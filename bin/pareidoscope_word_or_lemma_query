#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import operator
import os
import sqlite3

import networkx
from networkx.readwrite import json_graph
import scipy.stats

from pareidoscope import star_extraction
from pareidoscope import subgraph_enumeration
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph
from pareidoscope.utils import statistics

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def collect_frequencies(d, e, s, m, o11ids, r1ids, o11, r1, o11_to_r1):
    """"""
    # q1 = "SELECT ese.classid FROM edge_star_elements AS ese JOIN edge_stars AS es ON ese.elemid = es.starid WHERE es.subgraph=? AND es.center=?"
    # q2 = "SELECT sse.classid FROM skel_star_elements AS sse JOIN skel_stars AS ss ON sse.elemid = ss.starid WHERE ss.subgraph=? AND ss.center=?"
    # get_classid = lambda q, t: d.execute(q, t).fetchall()[0][0]
    q1 = "SELECT ese.classid, esc.isomorphisms_per_star FROM edge_star_elements AS ese JOIN edge_stars AS es ON ese.elemid = es.starid JOIN edge_star_classes AS esc ON ese.classid = esc.classid WHERE es.subgraph=? AND es.center=?"
    q2 = "SELECT sse.classid, ssc.isomorphisms_per_star FROM skel_star_elements AS sse JOIN skel_stars AS ss ON sse.elemid = ss.starid JOIN skel_star_classes AS ssc ON sse.classid = ssc.classid WHERE ss.subgraph=? AND ss.center=?"
    get_classid_and_iso_per_star = lambda q, t: d.execute(q, t).fetchall()[0]
    for o in e:
        if o not in o11ids:
            o11ids[o] = get_classid_and_iso_per_star(q1, o)
        classid = o11ids[o][0]
        o11["stars"][classid] += e[o]["star_freq"]
        o11["centers"][classid] += e[o]["center_freq"]
        o11["isomorphisms"][classid] += o11ids[o][1] * e[o]["star_freq"]
    for r in s:
        if r not in r1ids:
            r1ids[r] = get_classid_and_iso_per_star(q2, r)
        classid = r1ids[r][0]
        r1["stars"][classid] += s[r]["star_freq"]
        r1["centers"][classid] += s[r]["center_freq"]
        r1["isomorphisms"][classid] += r1ids[r][1] * s[r]["star_freq"]
    for o in m:
        oid = o11ids[o][0]
        rids = list(set([r1ids[r][0] for r in m[o]]))
        if len(rids) != 1:
            raise Exception("More than one skel_star for edge_star %s: %s" % (oid, ", ".join(rids)))
        if oid in o11_to_r1:
            if o11_to_r1[oid] != rids[0]:
                raise Exception("More than one skel_star for edge_star %s: %s" % (oid, ", ".join([o11_to_r1[oid], rids[0]])))
        else:
            o11_to_r1[oid] = rids[0]


def get_frequencies_from_db(c, o11, r1, c1, n, lemma, tag):
    """"""
    q1 = "SELECT isomorphisms_per_star, stars, star_centers FROM edge_star_classes WHERE classid=?"
    q2 = "SELECT isomorphisms_per_star, stars, star_centers FROM skel_star_classes WHERE classid=?"
    pos_or_wc = "wc" if lemma else "pos"
    q3 = "SELECT sum(xes.star_centers) FROM %s_edge_stars AS xes JOIN edge_star_elements AS ese ON xes.starid=ese.elemid WHERE ese.classid=? AND xes. %s=?" % (pos_or_wc, pos_or_wc)
    for o in o11["stars"]:
        isomorphisms_per_star, stars, star_centers = c.execute(q1, (o,)).fetchone()
        c1["stars"][o] += stars
        c1["centers"][o] += star_centers
        c1["isomorphisms"][o] += isomorphisms_per_star * stars
        c1["collostructional"][o] += c.execute(q3, (o, tag)).fetchall()[0][0]
    for r in r1["stars"]:
        isomorphisms_per_star, stars, star_centers = c.execute(q2, (r,)).fetchone()
        n["stars"][r] += stars
        n["centers"][r] += star_centers
        n["isomorphisms"][r] += isomorphisms_per_star * stars


def calculate_associations(o11, r1, c1, n, o11_to_r1, measure=statistics.log_likelihood):
    """"""
    associations = {}
    for count_method in o11.keys():
        for o11_star in o11[count_method]:
            if o11_star not in associations:
                associations[o11_star] = {}
            f_o11 = o11[count_method][o11_star]
            f_c1 = c1[count_method][o11_star]
            r1_star = o11_to_r1[o11_star]
            f_r1, f_n = None, None
            try:
                f_r1 = r1[count_method][r1_star]
                f_n = n[count_method][r1_star]
            except TypeError:
                f_r1 = r1[count_method]
                f_n = n[count_method]
            o, e = statistics.get_contingency_table(f_o11, f_r1, f_c1, f_n)
            associations[o11_star][count_method] = measure(o, e)
    return associations


def connect_to_results_db(filename):
    """"""
    conn = sqlite3.connect(filename)
    c = conn.cursor()
    c.execute("PRAGMA page_size=4096")
    c.execute("PRAGMA cache_size=100000")
    c.execute("CREATE TABLE IF NOT EXISTS forms (formid INTEGER PRIMARY KEY AUTOINCREMENT, form TEXT, tag TEXT, islemma BOOLEAN, UNIQUE (form, tag, islemma))")
    c.execute("CREATE TABLE IF NOT EXISTS stars (starid INTEGER PRIMARY KEY, star TEXT, center INTEGER, UNIQUE (star, center))")
    c.execute("CREATE TABLE IF NOT EXISTS associations (formid INTEGER, starid INTEGER, counting-method TEXT, dice REAL, t-score REAL, log-likelihood REAL, o11 INTEGER, r1 INTEGER, c1 INTEGER, n INTEGER, UNIQUE(formid, starid, counting-method), FOREIGN KEY (formid) REFERENCES forms, FOREIGN KEY (starid) REFERENCES stars")
    c.execute("CREATE TABLE IF NOT EXISTS correlations (formid INTEGER, counting-method-1 TEXT, counting-method-2 TEXT, rho-dice REAL, rho-t-score REAL, rho-log-likelihood REAL, UNIQUE (formid, counting-method-1, counting-method-2), FOREIGN KEY (formid) REFERENCES forms)")
    return conn, c

    

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run a batch of queries against a corpus')
    parser.add_argument("--db", type=os.path.abspath, required=True, help="Corpus database")
    # parser.add_argument("-o", "--output", type=str, required=True, help="Output prefix")
    parser.add_argument("-l", "--lemma", action="store_true", help="FORM is a lemma and TAG is a word class")
    parser.add_argument("FORM", type=str, help="Word form or lemma")
    parser.add_argument("TAG", type=str, help="part-of-speech or word-class tag")
    args = parser.parse_args()
    
    groupsize = 50 * 10 * multiprocessing.cpu_count()
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    
    logging.info("connect to database '%s'" % args.db)
    conn = sqlite3.connect(args.db)
    c = conn.cursor()
    c.execute("PRAGMA cache_size=100000")
    # conn.create_function("REGEXP", 2, database.regexp)
    
    logging.info("collect stars")
    form = "lemma" if args.lemma else "word"
    tag = "wc" if args.lemma else "pos"
    q = "SELECT tokens.sentid, sentences.graph, tokens.position FROM tokens INNER JOIN types ON types.typeid = tokens.typeid INNER JOIN sentences ON sentences.sentid = tokens.sentid WHERE %s=? AND %s=?" % (form, tag)
    q_r1 = "SELECT count(*) FROM tokens INNER JOIN types ON types.typeid = tokens.typeid WHERE %s=? AND %s=?" % (form, tag)
    q_n = "SELECT count(*) FROM tokens INNER JOIN types ON types.typeid = tokens.typeid WHERE %s=?" % tag
    t = (args.FORM, args.TAG)
    o11 = {"stars": collections.Counter(), "centers": collections.Counter(), "isomorphisms": collections.Counter()}
    o11["collostructional"] = o11["centers"]
    r1 = {"stars": collections.Counter(), "centers": collections.Counter(), "isomorphisms": collections.Counter(), "collostructional": 0}
    c1 = {"stars": collections.Counter(), "centers": collections.Counter(), "isomorphisms": collections.Counter(), "collostructional": collections.Counter()}
    n = {"stars": collections.Counter(), "centers": collections.Counter(), "isomorphisms": collections.Counter(), "collostructional": 0}
    o11_to_r1 = {}
    for occurrences in helper.grouper_nofill(groupsize, c.execute(q, t)):
        o11ids, r1ids = {}, {}
        d = conn.cursor()
        r = pool.imap_unordered(star_extraction.extract_stars_for_position, occurrences, 10)
        # r = map(star_extraction.extract_stars_for_position, occurrences)
        for sentid, e, s, m in r:
            collect_frequencies(d, e, s, m, o11ids, r1ids, o11, r1, o11_to_r1)
    logging.info("get frequencies from database")
    get_frequencies_from_db(c, o11, r1, c1, n, args.lemma, args.TAG)
    r1["collostructional"] = c.execute(q_r1, t).fetchall()[0][0]
    n["collostructional"] = c.execute(q_n, (args.TAG,)).fetchall()[0][0]
    logging.info("calculate association strengths and correlations")
    dice_associations = calculate_associations(o11, r1, c1, n, o11_to_r1, statistics.dice)
    tscore_associations = calculate_associations(o11, r1, c1, n, o11_to_r1, statistics.t_score)
    ll_associations = calculate_associations(o11, r1, c1, n, o11_to_r1, statistics.log_likelihood)
    print scipy.stats.spearmanr([[v["collostructional"], v["centers"], v["stars"], v["isomorphisms"]] for v in dice_associations.itervalues()])[0]
    print scipy.stats.spearmanr([[v["collostructional"], v["centers"], v["stars"], v["isomorphisms"]] for v in tscore_associations.itervalues()])[0]
    print scipy.stats.spearmanr([[v["collostructional"], v["centers"], v["stars"], v["isomorphisms"]] for v in ll_associations.itervalues()])[0]
    logging.info("done")
    # for star, assoc in sorted(associations.iteritems(), key=lambda t: t[1]["centers"]):
    #     print star, assoc


