#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import sqlite3
import os

import networkx
from networkx.readwrite import json_graph

from pareidoscope import subgraph_enumeration
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def extract_stars(args):
    """Extract subgraphs from sentence
    
    Arguments:
    - `sentid`:
    - `position`:
    - `graph`:
    """
    sentid, position, graph = args
    edge_stars = collections.Counter()
    skel_stars = collections.Counter()
    edge_to_skel = {}
    gs = json_graph.node_link_graph(json.loads(graph))
    bfo_graph, bfo_to_raw = subgraph_enumeration.get_bfo(gs)
    raw_to_bfo = {v: k for k, v in bfo_to_raw.iteritems()}
    bfo_to_bfo = {v: v for v in bfo_graph.nodes()}
    vertice = raw_to_bfo[position]
    whole_star = bfo_graph.subgraph(set([vertice] + bfo_graph.predecessors(vertice) + bfo_graph.successors(vertice)))
    for subgraph in subgraph_enumeration.enumerate_csg_minmax(whole_star, bfo_to_bfo, min_vertices=2, max_vertices=whole_star.__len__()):
        # vertice must be in subgraph
        if not subgraph.has_node(vertice):
            continue
        # vertice must be center of star
        other_vertices = set(subgraph.nodes()) - set([vertice])
        if len(other_vertices) == 0:
            continue
        if not all((subgraph.has_edge(vertice, x) or subgraph.has_edge(x, vertice) for x in other_vertices)):
            continue
        edge_star = subgraph.copy()
        nx_graph.skeletize_inplace(edge_star, only_vertices=True)
        edge_star, order = nx_graph.canonize(edge_star, order=True)
        edge_center = order.index(vertice)
        edge_t = tuple(networkx.generate_edgelist(edge_star, data=True))
        edge_string = json.dumps(list(edge_t))
        edge_stars[(edge_string, edge_center)] += 1
        skel_star = subgraph.copy()
        nx_graph.skeletize_inplace(skel_star)
        skel_star, order = nx_graph.canonize(skel_star, order=True)
        skel_center = order.index(vertice)
        skel_t = tuple(networkx.generate_edgelist(skel_star))
        skel_string = json.dumps(list(skel_t))
        skel_stars[(skel_string, skel_center)] += 1
        if (edge_string, edge_center) not in edge_to_skel:
            edge_to_skel[(edge_string, edge_center)] = set([(skel_string, skel_center)])
        else:
            edge_to_skel[(edge_string, edge_center)].add((skel_string, skel_center))
    return sentid, edge_stars, skel_stars, edge_to_skel





if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run a batch of queries against a corpus')
    parser.add_argument("--db", type=os.path.abspath, required=True, help="Corpus database")
    # parser.add_argument("-o", "--output", type=str, required=True, help="Output prefix")
    parser.add_argument("-l", "--lemma", action="store_true", help="FORM is a lemma and TAG is a word class")
    parser.add_argument("FORM", type=str, help="Word form or lemma")
    parser.add_argument("TAG", type=str, help="part-of-speech or word-class tag")
    args = parser.parse_args()
    
    groupsize = 50 * 10 * multiprocessing.cpu_count()
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    
    logging.info("connect to database '%s'" % args.db)
    conn = sqlite3.connect(args.db)
    c = conn.cursor()
    c.execute("PRAGMA cache_size=100000")
    # conn.create_function("REGEXP", 2, database.regexp)
    
    logging.info("collect stars")
    form = "lemma" if args.lemma else "word"
    tag = "wc" if args.lemma else "pos"
    q = "SELECT tokens.sentid, tokens.position, sentences.graph FROM tokens INNER JOIN types ON types.typeid = tokens.typeid INNER JOIN sentences ON sentences.sentid = tokens.sentid WHERE %s=?" % form
    t = (args.FORM,)
    if args.TAG:
        q += " AND %s=?" % tag
        t = (args.FORM, args.TAG)
    o11_stars = collections.Counter()
    r1_stars = collections.Counter()
    o11_centers = collections.Counter()
    r1_centers = collections.Counter()
    o11_to_r1 = {}
    for occurrences in helper.grouper_nofill(groupsize, c.execute(q, t)):
        d = conn.cursor()
        r = pool.imap_unordered(extract_stars, occurrences, 10)
        # r = map(extract_stars, occurrences)
        for sentid, e, s, m in r:
            q1 = "SELECT ese.classid FROM edge_star_elements AS ese JOIN edge_stars AS es ON ese.elemid = es.starid WHERE es.subgraph=? AND es.center=?"
            q2 = "SELECT sse.classid FROM skel_star_elements AS sse JOIN skel_stars AS ss ON sse.elemid = ss.starid WHERE ss.subgraph=? AND ss.center=?"
            get_classid = lambda q, t: d.execute(q, t).fetchall()[0][0]
            for o11 in e:
                print o11
                print d.execute(q1, o11).fetchall()
                classid = get_classid(q1, o11)
                o11_stars[classid] += e[o11]
                o11_centers[classid] += 1
            for r1 in s:
                classid = get_classid(q2, r1)
                r1_stars[classid] += s[r1]
                r1_centers[classid] += 1
            # o11_stars += e
            # r1_stars += s
            # o11_centers += collections.Counter({k: 1 for k in e})
            # r1_centers += collections.Counter({k: 1 for k in s})
    print r1_stars
    logging.info("calculate association strengths")
    c1_stars = collections.Counter()
    n_stars = collections.Counter()
    c1_centers = collections.Counter()
    n_centers = collections.Counter()
    # collapse isomorphic stars


    # queries = read_queries(args.QUERIES)
    # results = [{} for q in queries]
    # logging.info("%d queries", len(queries))
    # for query_number, qline in enumerate(queries):
    #     logging.info("process query %d", query_number+1)
    #     go11, ga, gb, gr1, gc1, gn = qline
    #     candidates = None
    #     if nx_graph.is_purely_structural(gn) and subgraphs:
    #         logging.info("get structure candidates")
    #         candidates = database.get_structure_candidates(c, gn)
    #     else:
    #         logging.info("get candidates")
    #         candidates = database.get_candidates(c, gn)
    #     logging.info("%d candidate sentences", len(candidates))
    #     for sentids in helper.grouper_nofill(groupsize, candidates):
    #         logging.info("process %d sentences", len(sentids))
    #         q = "SELECT graph FROM sentences WHERE sentid=?"
    #         sentences = [c.execute(q, (sentid,)).fetchone()[0] for sentid in sentids]
    #         cands = [candidates[sentid] for sentid in sentids]
    #         arguments = itertools.izip(itertools.repeat(go11), itertools.repeat(ga), itertools.repeat(gb), itertools.repeat(gr1), itertools.repeat(gc1), itertools.repeat(gn), sentences, cands)
    #         r = pool.imap(query.run_queries_db, arguments, 10)
    #         # r = map(query.run_queries_db, args)
    #         for result in r:
    #             query.merge_result_db(result, query_number, results)
    # logging.info("write results")
    # write_results(args.output, results)
