#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import os
import sqlite3
import sys
import tempfile

import networkx
from networkx.readwrite import json_graph

from pareidoscope import subgraph_enumeration
from pareidoscope.utils import cwb
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def create_db(filename, no_subgraphs):
    """Create an empty database
    
    Arguments:
    - `filename`:
    """
    # SELECT sentid, position FROM tokens INNER JOIN types USING (typeid) INNER JOIN sentences USING (sentid) INNER JOIN indeps USING (typeid) INNER JOIN outdeps USING (typeid) WHERE â€¦
    conn = sqlite3.connect(filename)
    c = conn.cursor()
    c.execute("PRAGMA page_size=4096")
    c.execute("PRAGMA cache_size=100000")
    ## tables for finding vertice candidates
    # types
    c.execute("CREATE TABLE types (typeid INTEGER PRIMARY KEY AUTOINCREMENT, word TEXT, pos TEXT, lemma TEXT, wc TEXT, root INTEGER, indeg INTEGER, outdeg INTEGER)")
    c.execute("CREATE INDEX types_word_idx ON types (word)")
    c.execute("CREATE INDEX types_pos_idx ON types (pos)")
    c.execute("CREATE INDEX types_lemma_idx ON types (lemma)")
    c.execute("CREATE INDEX types_wc_idx ON types (wc)")
    c.execute("CREATE INDEX types_root_idx ON types (root)")
    c.execute("CREATE INDEX types_indeg_idx ON types (indeg)")
    c.execute("CREATE INDEX types_outdeg_idx ON types (outdeg)")
    # indeps
    c.execute("CREATE TABLE indeps (typeid INTEGER, indep TEXT, FOREIGN KEY (typeid) REFERENCES types, UNIQUE (typeid, indep))")
    c.execute("CREATE INDEX indeps_idx ON indeps (indep)")
    # outdeps
    c.execute("CREATE TABLE outdeps (typeid INTEGER, outdep TEXT, FOREIGN KEY (typeid) REFERENCES types, UNIQUE (typeid, outdep))")
    c.execute("CREATE INDEX outdeps_idx ON outdeps (outdep)")
    # sentences
    c.execute("CREATE TABLE sentences (sentid INTEGER PRIMARY KEY AUTOINCREMENT, origid TEXT, graph TEXT)")
    # tokens
    c.execute("CREATE TABLE tokens (typeid INTEGER, sentid INTEGER, position INTEGER, FOREIGN KEY (typeid) REFERENCES types, FOREIGN KEY (sentid) REFERENCES sentences, UNIQUE (typeid, sentid, position))")
    if no_subgraphs:
        return conn, c
    ## tables for abstract subgraphs
    c.execute("CREATE TABLE subgraphs (subgraphid INTEGER PRIMARY KEY AUTOINCREMENT, subgraph TEXT UNIQUE, length INTEGER, isomorphisms INTEGER)")
    # size 2
    c.execute("CREATE TABLE subgraphs2 (subgraphid INTEGER, sentid INTEGER, v1 INTEGER, v2 INTEGER, FOREIGN KEY (subgraphid) REFERENCES subgraphs)")
    c.execute("CREATE INDEX subgraphs2_idx ON subgraphs2 (subgraphid)")
    # size 3
    c.execute("CREATE TABLE subgraphs3 (subgraphid INTEGER, sentid INTEGER, v1 INTEGER, v2 INTEGER, v3 INTEGER, FOREIGN KEY (subgraphid) REFERENCES subgraphs)")
    c.execute("CREATE INDEX subgraphs3_idx ON subgraphs3 (subgraphid)")
    # size 4
    c.execute("CREATE TABLE subgraphs4 (subgraphid INTEGER, sentid INTEGER, v1 INTEGER, v2 INTEGER, v3 INTEGER, v4 INTEGER, FOREIGN KEY (subgraphid) REFERENCES subgraphs)")
    c.execute("CREATE INDEX subgraphs4_idx ON subgraphs4 (subgraphid)")
    # size 5
    c.execute("CREATE TABLE subgraphs5 (subgraphid INTEGER, sentid INTEGER, v1 INTEGER, v2 INTEGER, v3 INTEGER, v4 INTEGER, v5 INTEGER, FOREIGN KEY (subgraphid) REFERENCES subgraphs)")
    c.execute("CREATE INDEX subgraphs5_idx ON subgraphs5 (subgraphid)")
    return conn, c


def tupleize(stuple):
    """Return a list of tuples representing the tokens
    
    Arguments:
    - `sentence`:
    """
    sentence, origid = stuple
    gs = nx_graph.create_nx_digraph_from_cwb(sentence, origid)
    sensible = nx_graph.is_sensible_graph(gs)
    result = {}
    result["tokens"] = {}
    result["graph"] = json.dumps(json_graph.node_link_data(gs))
    result["origid"] = origid
    for v in gs.nodes():
        word = gs.node[v]["word"]
        pos = gs.node[v]["pos"]
        lemma = gs.node[v]["lemma"]
        wc = gs.node[v]["wc"]
        root = 1 if "root" in gs.node[v] else 0
        indegree = gs.in_degree(v)
        outdegree = gs.out_degree(v)
        indeps = set([gs.edge[s][t]["relation"] for s, t in gs.in_edges(v)])
        indeps = tuple(sorted(list(indeps)))
        outdeps = set([gs.edge[s][t]["relation"] for s, t in gs.out_edges(v)])
        outdeps = tuple(sorted(list(outdeps)))
        t = (word, pos, lemma, wc, root, indegree, outdegree, indeps, outdeps)
        result["tokens"][v] = t
    return result, sensible


def preprocess_results(conn, c, result, results):
    """Collect results
    
    Arguments:
    - `result`:
    - `results`:
    """
    c.execute("INSERT INTO sentences (origid, graph) VALUES (?, ?)", (result["origid"], result["graph"],))
    sentid = c.lastrowid
    for position, t in result["tokens"].iteritems():
        if t not in results:
            results[t] = []
        results[t].append([sentid, position])


def insert_tokens(conn, c, results):
    """Insert token-level data into database
    
    Arguments:
    - `conn`:
    - `c`:
    - `results`:
    """
    for t in results:
        c.execute("INSERT INTO types (word, pos, lemma, wc, root, indeg, outdeg) VALUES (?,?,?,?,?,?,?)", t[0:7])
        typeid = c.lastrowid
        indeps = itertools.izip(itertools.repeat(typeid), t[7])
        c.executemany("INSERT INTO indeps VALUES (?,?)", indeps)
        outdeps = itertools.izip(itertools.repeat(typeid), t[8])
        c.executemany("INSERT INTO outdeps VALUES (?,?)", outdeps)
        tokens = itertools.izip(itertools.repeat([typeid]), results[t])
        tokens = (tuple(itertools.chain.from_iterable(token)) for token in tokens)
        c.executemany("INSERT INTO tokens VALUES (?,?,?)", tokens)
    conn.commit()


def extract_subgraphs(args):
    """Extract subgraphs from sentence
    
    Arguments:
    - `sentid`:
    - `graph`:
    """
    sentid, graph = args
    result = []
    gs = json_graph.node_link_graph(json.loads(graph))
    bfo_graph, bfo_to_raw = subgraph_enumeration.get_bfo(gs)
    for subgraph in subgraph_enumeration.enumerate_csg_minmax(bfo_graph, bfo_to_raw):
        nx_graph.skeletize_inplace(subgraph)
        subgraph, order = nx_graph.canonize(subgraph, order=True)
        t = tuple(networkx.generate_adjlist(subgraph))
        result.append([t, sentid, order])
    return result


def insert_subgraphs(conn, c, result, subgraphs):
    """Insert subgraphs into database
    
    Arguments:
    - `conn`:
    - `c`:
    - `result`:
    - `subgraphs`:
    """
    for r in result:
        sg, sentid, positions = r
        if sg not in subgraphs["id"]:
            graph = networkx.parse_adjlist(sg, nodetype=int, create_using=networkx.DiGraph())
            # check if sg is isomorphic to any known subgraph
            isomorphic = [t for t, g in subgraphs["graph"].iteritems() if networkx.is_isomorphic(g, graph)]
            if len(isomorphic) == 0:
                c.execute("INSERT INTO subgraphs (subgraph, length) VALUES (?,?)", (json.dumps(list(sg)),len(sg)))
                subgraphs["graph"][sg] = graph
                subgraphs["id"][sg] = c.lastrowid
            else:
                if len(isomorphic) > 1:
                    raise Exception("There are already more than one isomorphic subgraphs in the database!")
                subgraphs["id"][sg] = subgraphs["id"][isomorphic[0]]
        subgraphid = subgraphs["id"][sg]
        subgraphs["isomorphisms"][subgraphid] = subgraphs["isomorphisms"].get(subgraphid, 0) + 1
        length = len(sg)
        if length > 5 or length < 2:
            raise Exception("Subgraph of length %d: %s" % (length, subgraph))
        query = "INSERT INTO subgraphs%d (subgraphid, sentid, %s) VALUES (?,?,%s)" % (length, ", ".join("v%d" % i for i in range(1, length + 1)), ",".join(["?"] * length))
        params = [subgraphid, sentid]
        params.extend(positions)
        c.execute(query, tuple(params))



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Convert a corpus in CWB format to an SQLite database')
    # parser.add_argument("--min-subgraph", type=int, help="Minimum subgraph size for indexing (default: 2 vertices)", default=2, metavar="N")
    # parser.add_argument("--max-subgraph", type=int, help="Maximum subgraph size for indexing (default: 5 vertices)", default=5, metavar="N")
    parser.add_argument("--no-subgraphs", action="store_true", help="Do not create subgraph indexes")
    parser.add_argument("--db", type=os.path.abspath, required=True, help="SQLite3 database")
    parser.add_argument("CORPUS", type=argparse.FileType("r"), help="Corpus in CWB format")
    args = parser.parse_args()

    groupsize = 50 * 10 * multiprocessing.cpu_count()
    # raise exeception if args.db already exists
    if os.path.exists(args.db):
        raise Exception("Database file already exists")
    conn, c = create_db(args.db, args.no_subgraphs)
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    sents = cwb.sentences_iter(args.CORPUS, return_id=True)
    results = {}
    logging.info("tupleize and preprocess")
    for sentences in helper.grouper_nofill(groupsize, sents):
        r = pool.imap_unordered(tupleize, sentences, 10)
        #r = map(tupleize, sentences)
        for result, sensible in r:
            if sensible:
                preprocess_results(conn, c, result, results)
    logging.info("insert tokens")
    insert_tokens(conn, c, results)
    if args.no_subgraphs:
        conn.commit()
        conn.close()
        sys.exit()
    subgraphs = {"isomorphisms": {}, "id": {}, "graph": {}}
    logging.info("exctract subgraphs")
    sent_count = c.execute("SELECT count(*) FROM sentences").fetchone()[0]
    logging.info("%d sentences" % sent_count)
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        r = pool.imap_unordered(extract_subgraphs, sentences, 10)
        #r = map(extract_subgraphs, sentences)
        for result in r:
            insert_subgraphs(conn, c, result, subgraphs)
    isomorphisms = ((isos, sgid) for sgid, isos in subgraphs["isomorphisms"].iteritems())
    logging.info("insert isomorphism counts")
    c.executemany("UPDATE subgraphs SET isomorphisms=? WHERE subgraphid=?", isomorphisms)
    conn.commit()
    conn.close()
