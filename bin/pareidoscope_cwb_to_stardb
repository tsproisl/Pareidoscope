#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import os
import sqlite3
import sys
import tempfile

import networkx
from networkx.algorithms import isomorphism
from networkx.readwrite import json_graph

from pareidoscope import subgraph_enumeration
from pareidoscope.utils import cwb
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph

# logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)
logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.DEBUG)


def create_db(filename):
    """Create an empty database
    
    Arguments:
    - `filename`:
    """
    # SELECT sentid, position FROM tokens INNER JOIN types USING (typeid) INNER JOIN sentences USING (sentid) INNER JOIN indeps USING (typeid) INNER JOIN outdeps USING (typeid) WHERE â€¦
    conn = sqlite3.connect(filename)
    c = conn.cursor()
    c.execute("PRAGMA page_size=4096")
    c.execute("PRAGMA cache_size=100000")
    ## tables for finding vertice candidates
    # types
    c.execute("CREATE TABLE types (typeid INTEGER PRIMARY KEY AUTOINCREMENT, word TEXT, pos TEXT, lemma TEXT, wc TEXT, root INTEGER, indeg INTEGER, outdeg INTEGER)")
    c.execute("CREATE INDEX types_word_idx ON types (word)")
    c.execute("CREATE INDEX types_pos_idx ON types (pos)")
    c.execute("CREATE INDEX types_lemma_idx ON types (lemma)")
    c.execute("CREATE INDEX types_wc_idx ON types (wc)")
    c.execute("CREATE INDEX types_root_idx ON types (root)")
    c.execute("CREATE INDEX types_indeg_idx ON types (indeg)")
    c.execute("CREATE INDEX types_outdeg_idx ON types (outdeg)")
    # indeps
    c.execute("CREATE TABLE indeps (typeid INTEGER, indep TEXT, FOREIGN KEY (typeid) REFERENCES types, UNIQUE (typeid, indep))")
    c.execute("CREATE INDEX indeps_idx ON indeps (indep)")
    # outdeps
    c.execute("CREATE TABLE outdeps (typeid INTEGER, outdep TEXT, FOREIGN KEY (typeid) REFERENCES types, UNIQUE (typeid, outdep))")
    c.execute("CREATE INDEX outdeps_idx ON outdeps (outdep)")
    # sentences
    c.execute("CREATE TABLE sentences (sentid INTEGER PRIMARY KEY AUTOINCREMENT, origid TEXT, graph TEXT)")
    # tokens
    c.execute("CREATE TABLE tokens (typeid INTEGER, sentid INTEGER, position INTEGER, FOREIGN KEY (typeid) REFERENCES types, FOREIGN KEY (sentid) REFERENCES sentences, UNIQUE (typeid, sentid, position))")
    ## tables for stars
    c.execute("CREATE TABLE edge_stars (starid INTEGER PRIMARY KEY AUTOINCREMENT, subgraph TEXT UNIQUE, length INTEGER, stars INTEGER, star_centers INTEGER)")
    c.execute("CREATE TABLE skel_stars (starid INTEGER PRIMARY KEY AUTOINCREMENT, subgraph TEXT UNIQUE, length INTEGER, stars INTEGER, star_centers INTEGER)")
    return conn, c


def tupleize(stuple):
    """Return a list of tuples representing the tokens
    
    Arguments:
    - `sentence`:
    """
    sentence, origid = stuple
    gs = nx_graph.create_nx_digraph_from_cwb(sentence, origid)
    sensible = nx_graph.is_sensible_graph(gs)
    result = {}
    result["tokens"] = {}
    result["graph"] = json.dumps(json_graph.node_link_data(gs))
    result["origid"] = origid
    for v in gs.nodes():
        word = gs.node[v]["word"]
        pos = gs.node[v]["pos"]
        lemma = gs.node[v]["lemma"]
        wc = gs.node[v]["wc"]
        root = 1 if "root" in gs.node[v] else 0
        indegree = gs.in_degree(v)
        outdegree = gs.out_degree(v)
        indeps = set([gs.edge[s][t]["relation"] for s, t in gs.in_edges(v)])
        indeps = tuple(sorted(list(indeps)))
        outdeps = set([gs.edge[s][t]["relation"] for s, t in gs.out_edges(v)])
        outdeps = tuple(sorted(list(outdeps)))
        t = (word, pos, lemma, wc, root, indegree, outdegree, indeps, outdeps)
        result["tokens"][v] = t
    return result, sensible


def preprocess_results(conn, c, result, results):
    """Collect results
    
    Arguments:
    - `result`:
    - `results`:
    """
    c.execute("INSERT INTO sentences (origid, graph) VALUES (?, ?)", (result["origid"], result["graph"],))
    sentid = c.lastrowid
    for position, t in result["tokens"].iteritems():
        if t not in results:
            results[t] = []
        results[t].append([sentid, position])


def insert_tokens(conn, c, results):
    """Insert token-level data into database
    
    Arguments:
    - `conn`:
    - `c`:
    - `results`:
    """
    for t in results:
        c.execute("INSERT INTO types (word, pos, lemma, wc, root, indeg, outdeg) VALUES (?,?,?,?,?,?,?)", t[0:7])
        typeid = c.lastrowid
        indeps = itertools.izip(itertools.repeat(typeid), t[7])
        c.executemany("INSERT INTO indeps VALUES (?,?)", indeps)
        outdeps = itertools.izip(itertools.repeat(typeid), t[8])
        c.executemany("INSERT INTO outdeps VALUES (?,?)", outdeps)
        tokens = itertools.izip(itertools.repeat([typeid]), results[t])
        tokens = (tuple(itertools.chain.from_iterable(token)) for token in tokens)
        c.executemany("INSERT INTO tokens VALUES (?,?,?)", tokens)
    conn.commit()


def extract_stars(args):
    """Extract subgraphs from sentence
    
    Arguments:
    - `sentid`:
    - `graph`:
    """
    sentid, graph = args
    edge_stars = collections.Counter()
    skel_stars = collections.Counter()
    gs = json_graph.node_link_graph(json.loads(graph))
    bfo_graph, bfo_to_raw = subgraph_enumeration.get_bfo(gs)
    for vertice in bfo_graph.nodes():
        whole_star = gs.subgraph([vertice])
        for subgraph in subgraph_enumeration.enumerate_csg_minmax(whole_star, bfo_to_raw, min_vertices=2, max_vertices=whole_star.__len__()):
            # vertice must be in subgraph
            if not subgraph.has_node(vertice):
                continue
            # vertice must be center of star
            other_vertices = set(subgraph.nodes()).remove(vertice)
            if not all((subgraph.has_edge(vertice, x) or subgraph.has_edge(x, vertice) for x in other_vertices)):
                continue
            edge_star = subgraph.copy()
            nx_graph.skeletize_inplace(edge_star, only_vertices=True)
            edge_star, order = nx_graph.canonize(edge_star, order=True)
            edge_center = order.index(vertice)
            edge_t = tuple(networkx.generate_edgelist(edge_star, data=True))
            edge_stars[(edge_t, edge_center)] += 1
            skel_star = subgraph.copy()
            nx_graph.skeletize_inplace(skel_star)
            skel_star, order = nx_graph.canonize(skel_star, order=True)
            skel_center = order.index(vertice)
            skel_t = tuple(networkx.generate_edgelist(skel_star))
            skel_stars[(skel_t, skel_center)] += 1
            # result.append([sentid, edge_t, edge_center, skel_t, skel_center])
    return sentid, edge_stars, skel_stars


def insert_stars(conn, c, result, stars):
    """Insert subgraphs into database
    
    Arguments:
    - `conn`:
    - `c`:
    - `result`:
    - `subgraphs`:
    """
    sentid, edge_stars, skel_stars = result
    for varstr, variant in [("edge", edge_stars), ("skel", skel_stars)]:
        for star in variant:
            logging.debug(star)
            if star not in stars[varstr]["id"]:
                graph = networkx.parse_adjlist(star[0], nodetype=int, create_using=networkx.DiGraph())
                # check if star is isomorphic to any known subgraph
                # and if one of the isomorphisms maps the star centers
                # to each other
                dgm = isomorphism.DiGraphMatcher(graph, stars[varstr]["graph"][isomorphic[0]], edge_match=lambda x, y: x.get("relation", "") == y.get("relation", ""))
                if any(iso[star[1]] == isomorphic[0][1] for iso in dgm.isomorphisms_iter()):
                    stars[varstr]["id"][star] = stars[varstr]["id"][isomorphic[0]]
                else:
                    c.execute("INSERT INTO %s_stars (subgraph, length) VALUES (?,?)" % varstr, (json.dumps(list(star)),len(star[0])))
                    stars[varstr]["graph"][star] = graph
                    stars[varstr]["id"][star] = c.lastrowid
            starid = stars[varstr]["id"][star]
            stars[varstr]["star_centers"][starid] = stars[varstr]["star_centers"].get(starid, 0) + 1
            stars[varstr]["stars"][starid] = stars[varstr]["stars"].get(starid, 0) + variant[star]



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Convert a corpus in CWB format to an SQLite database and collect frequency information about stars')
    parser.add_argument("--db", type=os.path.abspath, required=True, help="SQLite3 database")
    parser.add_argument("CORPUS", type=argparse.FileType("r"), help="Corpus in CWB format")
    args = parser.parse_args()

    groupsize = 50 * 10 * multiprocessing.cpu_count()
    # raise exeception if args.db already exists
    if os.path.exists(args.db):
        raise Exception("Database file already exists")
    conn, c = create_db(args.db)
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    sents = cwb.sentences_iter(args.CORPUS, return_id=True)
    results = {}
    logging.info("tupleize and preprocess")
    for sentences in helper.grouper_nofill(groupsize, sents):
        r = pool.imap_unordered(tupleize, sentences, 10)
        #r = map(tupleize, sentences)
        for result, sensible in r:
            if sensible:
                preprocess_results(conn, c, result, results)
    logging.info("insert tokens")
    insert_tokens(conn, c, results)
    stars = {"edge": {"stars": {}, "star_centers": {}, "id": {}, "graph": {}}, "skel": {"stars": {}, "star_centers": {}, "id": {}, "graph": {}}}
    logging.info("exctract stars")
    sent_count = c.execute("SELECT count(*) FROM sentences").fetchone()[0]
    logging.info("%d sentences" % sent_count)
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        r = pool.imap_unordered(extract_stars, sentences, 10)
        #r = map(extract_stars, sentences)
        for result in r:
            insert_stars(conn, c, result, stars)
    for varstr in ["edge", "skel"]:
        stars_count = ((sc, sgid) for sgid, sc in stars[varstr]["stars"].iteritems())
        logging.info("insert %s stars counts" % varstr)
        c.executemany("UPDATE %s_stars SET stars=? WHERE starid=?" %varstr, stars_count)
        star_centers_count = ((sc, sgid) for sgid, sc in stars[varstr]["star_centers"].iteritems())
        logging.info("insert %s star centers counts" % varstr)
        c.executemany("UPDATE %s_stars SET star_centers=? WHERE starid=?" % varstr, star_centers_count)
    conn.commit()
    conn.close()
