#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import os
import sqlite3
import sys
import tempfile

import networkx
from networkx.algorithms import isomorphism
from networkx.readwrite import json_graph

from pareidoscope import star_extraction
from pareidoscope import subgraph_enumeration
from pareidoscope.utils import cwb
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)
# logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.DEBUG)


def create_db(filename):
    """Create an empty database
    
    Arguments:
    - `filename`:
    """
    # SELECT sentid, position FROM tokens INNER JOIN types USING (typeid) INNER JOIN sentences USING (sentid) INNER JOIN indeps USING (typeid) INNER JOIN outdeps USING (typeid) WHERE â€¦
    dirname = os.path.dirname(filename)
    conn = sqlite3.connect(filename)
    c = conn.cursor()
    c.execute("PRAGMA page_size=4096")
    c.execute("PRAGMA cache_size=100000")
    c.execute("PRAGMA temp_store=1")
    c.execute("PRAGMA temp_store_directory='%s'" % dirname)
    ## tables for finding vertice candidates
    # types
    c.execute("CREATE TABLE types (typeid INTEGER PRIMARY KEY AUTOINCREMENT, word TEXT, pos TEXT, lemma TEXT, wc TEXT, root INTEGER, indeg INTEGER, outdeg INTEGER)")
    c.execute("CREATE INDEX types_word_idx ON types (word)")
    c.execute("CREATE INDEX types_pos_idx ON types (pos)")
    c.execute("CREATE INDEX types_lemma_idx ON types (lemma)")
    c.execute("CREATE INDEX types_wc_idx ON types (wc)")
    c.execute("CREATE INDEX types_root_idx ON types (root)")
    c.execute("CREATE INDEX types_indeg_idx ON types (indeg)")
    c.execute("CREATE INDEX types_outdeg_idx ON types (outdeg)")
    # # indeps
    # c.execute("CREATE TABLE indeps (typeid INTEGER, indep TEXT, FOREIGN KEY (typeid) REFERENCES types, UNIQUE (typeid, indep))")
    # c.execute("CREATE INDEX indeps_idx ON indeps (indep)")
    # # outdeps
    # c.execute("CREATE TABLE outdeps (typeid INTEGER, outdep TEXT, FOREIGN KEY (typeid) REFERENCES types, UNIQUE (typeid, outdep))")
    # c.execute("CREATE INDEX outdeps_idx ON outdeps (outdep)")
    # sentences
    c.execute("CREATE TABLE sentences (sentid INTEGER PRIMARY KEY AUTOINCREMENT, origid TEXT, graph TEXT)")
    # tokens
    c.execute("CREATE TABLE tokens (typeid INTEGER, sentid INTEGER, position INTEGER, FOREIGN KEY (typeid) REFERENCES types, FOREIGN KEY (sentid) REFERENCES sentences, UNIQUE (typeid, sentid, position))")
    ## tables for stars
    c.execute("CREATE TABLE edge_stars (starid INTEGER PRIMARY KEY AUTOINCREMENT, subgraph TEXT, center INTEGER, length INTEGER, degree_sequence TEXT, edge_list TEXT, isomorphisms_per_star INTEGER, stars INTEGER, star_centers INTEGER, UNIQUE (subgraph, center))")
    c.execute("CREATE INDEX edegseq_idx ON edge_stars (degree_sequence)")
    c.execute("CREATE INDEX edge_list_idx ON edge_stars (edge_list)")
    c.execute("CREATE TABLE skel_stars (starid INTEGER PRIMARY KEY AUTOINCREMENT, subgraph TEXT, center INTEGER, length INTEGER, degree_sequence TEXT, isomorphisms_per_star INTEGER, stars INTEGER, star_centers INTEGER, UNIQUE (subgraph, center))")
    c.execute("CREATE INDEX sdegseq_idx ON skel_stars (degree_sequence)")
    c.execute("CREATE TABLE pos_edge_stars (starid INTEGER, pos TEXT, stars INTEGER, star_centers INTEGER, FOREIGN KEY (starid) REFERENCES edge_stars (starid), UNIQUE (starid, pos))")
    c.execute("CREATE TABLE wc_edge_stars (starid INTEGER, wc TEXT, stars INTEGER, star_centers INTEGER, FOREIGN KEY (starid) REFERENCES edge_stars (starid), UNIQUE (starid, wc))")
    # equivalence classes of isomorphic stars
    c.execute("CREATE TABLE edge_star_classes (classid INTEGER PRIMARY KEY AUTOINCREMENT, isomorphisms_per_star INTEGER, stars INTEGER, star_centers INTEGER)")
    c.execute("CREATE TABLE skel_star_classes (classid INTEGER PRIMARY KEY AUTOINCREMENT, isomorphisms_per_star INTEGER, stars INTEGER, star_centers INTEGER)")
    c.execute("CREATE TABLE edge_star_elements (classid INTEGER, elemid INTEGER, FOREIGN KEY (classid) REFERENCES edge_star_classes, FOREIGN KEY (elemid) REFERENCES edge_stars (starid), UNIQUE (elemid, classid))")
    c.execute("CREATE TABLE skel_star_elements (classid INTEGER, elemid INTEGER, FOREIGN KEY (classid) REFERENCES skel_star_classes, FOREIGN KEY (elemid) REFERENCES skel_stars (starid), UNIQUE (elemid, classid))")
    c.execute("CREATE INDEX classid_idx ON edge_star_elements (classid)")
    return conn, c


def tupleize(stuple):
    """Return a list of tuples representing the tokens
    
    Arguments:
    - `sentence`:
    """
    sentence, origid = stuple
    gs = nx_graph.create_nx_digraph_from_cwb(sentence, origid)
    sensible = nx_graph.is_sensible_graph(gs)
    result = {}
    result["tokens"] = {}
    result["graph"] = json.dumps(json_graph.node_link_data(gs), ensure_ascii=False, sort_keys=True)
    result["origid"] = origid
    for v in gs.nodes():
        word = gs.node[v]["word"]
        pos = gs.node[v]["pos"]
        lemma = gs.node[v]["lemma"]
        wc = gs.node[v]["wc"]
        root = 1 if "root" in gs.node[v] else 0
        indegree = gs.in_degree(v)
        outdegree = gs.out_degree(v)
        # indeps = set([gs.edge[s][t]["relation"] for s, t in gs.in_edges(v)])
        # indeps = tuple(sorted(list(indeps)))
        # outdeps = set([gs.edge[s][t]["relation"] for s, t in gs.out_edges(v)])
        # outdeps = tuple(sorted(list(outdeps)))
        # t = (word, pos, lemma, wc, root, indegree, outdegree, indeps, outdeps)
        t = (word, pos, lemma, wc, root, indegree, outdegree)
        result["tokens"][v] = t
    return result, sensible


def preprocess_results(conn, c, result, results):
    """Collect results
    
    Arguments:
    - `result`:
    - `results`:
    """
    c.execute("INSERT INTO sentences (origid, graph) VALUES (?, ?)", (result["origid"], result["graph"],))
    sentid = c.lastrowid
    for position, t in result["tokens"].iteritems():
        if t not in results:
            results[t] = []
        results[t].append([sentid, position])


def insert_tokens(conn, c, results):
    """Insert token-level data into database
    
    Arguments:
    - `conn`:
    - `c`:
    - `results`:
    """
    for t in results:
        c.execute("INSERT INTO types (word, pos, lemma, wc, root, indeg, outdeg) VALUES (?,?,?,?,?,?,?)", t[0:7])
        typeid = c.lastrowid
        # indeps = itertools.izip(itertools.repeat(typeid), t[7])
        # c.executemany("INSERT INTO indeps VALUES (?,?)", indeps)
        # outdeps = itertools.izip(itertools.repeat(typeid), t[8])
        # c.executemany("INSERT INTO outdeps VALUES (?,?)", outdeps)
        tokens = itertools.izip(itertools.repeat([typeid]), results[t])
        tokens = (tuple(itertools.chain.from_iterable(token)) for token in tokens)
        c.executemany("INSERT INTO tokens (typeid, sentid, position) VALUES (?,?,?)", tokens)
    conn.commit()


def _check_isomorphisms(star, graph, stars, varstr, candidates=[]):
    """Check if star is isomorphic to any known subgraph and if one of the
       isomorphisms maps the star centers to each other.

    """
    t_g_iterator = stars[varstr]["graph"].iteritems()
    if len(candidates) > 0:
        t_g_iterator = ((c, stars[varstr]["graph"][c]) for c in candidates)
    isomorphics = [t for t, g in t_g_iterator if networkx.is_isomorphic(g, graph, edge_match=lambda x, y: x.get("relation", "") == y.get("relation", ""))]
    for isomorphic in isomorphics:
        dgm = isomorphism.DiGraphMatcher(graph, stars[varstr]["graph"][isomorphic], edge_match=lambda x, y: x.get("relation", "") == y.get("relation", ""))
        if any(iso[star[1]] == isomorphic[1] for iso in dgm.isomorphisms_iter()):
            stars[varstr]["id"][star] = stars[varstr]["id"][isomorphic]
            logging.debug("%s is isomorphic to %s" % (star, isomorphic))
            return    


def insert_stars(c, result):
    """Insert subgraphs into database
    
    Arguments:
    - `c`:
    - `result`:

    """
    sentid, edge_stars, skel_stars = result
    for star in edge_stars:
        graph_string, center, pos, wc = star
        c.execute("INSERT OR IGNORE INTO edge_stars (subgraph, center, length, degree_sequence, edge_list, isomorphisms_per_star, stars, star_centers) VALUES (?, ?, ?, ?, ?, 0, 0, 0)", (graph_string, center, edge_stars[star]["length"], edge_stars[star]["degree_sequence"], edge_stars[star]["sorted_edges"]))
        starid = c.execute("SELECT starid FROM edge_stars WHERE subgraph = ? AND center = ?", (graph_string, center)).fetchall()[0][0]
        c.execute("UPDATE edge_stars SET stars = stars + ?, star_centers = star_centers + ? WHERE starid = ?", (edge_stars[star]["star_freq"], edge_stars[star]["center_freq"], starid))
        c.execute("INSERT OR IGNORE INTO pos_edge_stars (starid, pos, stars, star_centers) VALUES (?, ?, 0, 0)", (starid, pos))
        c.execute("INSERT OR IGNORE INTO wc_edge_stars (starid, wc, stars, star_centers) VALUES (?, ?, 0, 0)", (starid, wc))
        c.execute("UPDATE pos_edge_stars SET stars = stars + ?, star_centers = star_centers + ? WHERE starid = ? AND pos = ?", (edge_stars[star]["star_freq"], edge_stars[star]["center_freq"], starid, pos))
        c.execute("UPDATE wc_edge_stars SET stars = stars + ?, star_centers = star_centers + ? WHERE starid = ? AND wc = ?", (edge_stars[star]["star_freq"], edge_stars[star]["center_freq"], starid, wc))
    for star, star_freq in skel_stars.iteritems():
        graph_string, center = star
        c.execute("INSERT OR IGNORE INTO skel_stars (subgraph, center, length, degree_sequence, isomorphisms_per_star, stars, star_centers) VALUES (?, ?, ?, ?, 0, 0, 0)", (graph_string, center, skel_stars[star]["length"], skel_stars[star]["degree_sequence"]))
        c.execute("UPDATE skel_stars SET stars = stars + ?, star_centers = star_centers + ? WHERE subgraph = ? AND center = ?", (skel_stars[star]["star_freq"], skel_stars[star]["center_freq"], graph_string, center))


def iso_candidates_iter(conn, is_edge_star):
    """Yield a list of potentially isomorphic subgraphs.

    Arguments:
    - `conn`:
    - `c`:
    - `is_edge_star`:

    """
    c1 = conn.cursor()
    q1 = "SELECT DISTINCT degree_sequence FROM skel_stars"
    q2 = "SELECT starid, subgraph, center, stars, star_centers FROM skel_stars WHERE degree_sequence=?"
    if is_edge_star:
        q1 = "SELECT DISTINCT degree_sequence, edge_list FROM edge_stars"
        q2 = "SELECT starid, subgraph, center, stars, star_centers FROM edge_stars WHERE degree_sequence=? AND edge_list=?"
    for row in c1.execute(q1):
        c2 = conn.cursor()
        yield c2.execute(q2, row).fetchall()


def collapse_isomorphisms(args):
    """"""
    candidates, is_edge_star = args
    sets = {}
    frequencies = {}
    done = [False for _ in candidates]
    subgraph = None
    if is_edge_star:
        subgraph = [networkx.parse_edgelist(json.loads(c[1]), nodetype=int, create_using=networkx.DiGraph(), data=(("relation", unicode),)) for c in candidates]
    else:
        subgraph = [networkx.parse_edgelist(json.loads(c[1]), nodetype=int, create_using=networkx.DiGraph()) for c in candidates]
    for i in range(len(candidates)):
        if done[i]:
            continue
        sets[i] = set([candidates[i][0]])
        frequencies[i] = {"stars": candidates[i][3], "star_centers": candidates[i][4], "isomorphisms_per_star": star_extraction.count_isomorphisms(subgraph[i], subgraph[i], candidates[i][2], candidates[i][2])}
        done[i] = True
        for j in range(len(candidates)):
            if done[j]:
                continue
            dgm = isomorphism.DiGraphMatcher(subgraph[i], subgraph[j], edge_match=lambda x, y: x.get("relation", "") == y.get("relation", ""))
            if any(iso[int(candidates[i][2])] == int(candidates[j][2]) for iso in dgm.isomorphisms_iter()):
                sets[i].add(candidates[j][0])
                frequencies[i]["stars"] += candidates[j][3]
                frequencies[i]["star_centers"] += candidates[j][4]
                done[j] = True
                logging.debug("%s is isomorphic to %s" % (candidates[i][1], candidates[j][1]))
    return sets, frequencies


def insert_equivalence_classes(c, sets, frequencies, is_edge_star):
    """Insert equivalence classes of isomorphic stars into the database.

    """
    q1 = "INSERT INTO skel_star_classes (isomorphisms_per_star, stars, star_centers) VALUES (?, ?, ?)"
    q2 = "INSERT INTO skel_star_elements (classid, elemid) VALUES (?, ?)"
    q3 = "UPDATE skel_stars SET isomorphisms_per_star=? WHERE starid=?"
    if is_edge_star:
        q1 = "INSERT INTO edge_star_classes (isomorphisms_per_star, stars, star_centers) VALUES (?, ?, ?)"
        q2 = "INSERT INTO edge_star_elements (classid, elemid) VALUES (?, ?)"
        q3 = "UPDATE edge_stars SET isomorphisms_per_star=? WHERE starid=?"
    for i in sorted(sets.keys()):
        s = sets[i]
        isomorphisms_per_star = frequencies[i]["isomorphisms_per_star"]
        stars = frequencies[i]["stars"]
        star_centers = frequencies[i]["star_centers"]
        c.execute(q1, (isomorphisms_per_star, stars, star_centers))
        classid = c.lastrowid
        c.executemany(q2, zip(itertools.repeat(classid), s))
        c.executemany(q3, zip(itertools.repeat(isomorphisms_per_star), s))



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Convert a corpus in CWB format to an SQLite database and collect frequency information about stars')
    parser.add_argument("--db", type=os.path.abspath, required=True, help="SQLite3 database")
    parser.add_argument("CORPUS", type=argparse.FileType("r"), help="Corpus in CWB format")
    args = parser.parse_args()

    groupsize = 50 * 10 * multiprocessing.cpu_count()
    # raise exeception if args.db already exists
    if os.path.exists(args.db):
        raise Exception("Database file already exists")
    conn, c = create_db(args.db)
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    sents = cwb.sentences_iter(args.CORPUS, return_id=True)
    results = {}
    logging.info("tupleize and preprocess")
    for sentences in helper.grouper_nofill(groupsize, sents):
        r = pool.imap_unordered(tupleize, sentences, 10)
        # r = map(tupleize, sentences)
        for result, sensible in r:
            if sensible:
                preprocess_results(conn, c, result, results)
    logging.info("insert tokens")
    insert_tokens(conn, c, results)
    logging.info("exctract stars")
    sent_count = c.execute("SELECT count(*) FROM sentences").fetchone()[0]
    logging.info("%d sentences" % sent_count)
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        r = pool.imap_unordered(star_extraction.extract_all_stars, sentences, 10)
        # r = map(star_extraction.extract_all_stars, sentences)
        for result in r:
            insert_stars(c, result)
        conn.commit()
    logging.info("collapse isomorphic stars")
    for is_edge_star in [False, True]:
        logging.info("... %s" % ("edge stars" if is_edge_star else "skel_stars"))
        cands = iso_candidates_iter(conn, is_edge_star)
        for candidates in helper.grouper_nofill(groupsize, cands):
            r = pool.imap_unordered(collapse_isomorphisms, itertools.izip(candidates, itertools.repeat(is_edge_star)), 10)
            # r = map(collapse_isomorphisms, itertools.izip(candidates, itertools.repeat(is_edge_star)))
            for result in r:
                sets, frequencies = result
                insert_equivalence_classes(c, sets, frequencies, is_edge_star)
    conn.commit()
    conn.close()
    logging.info("done")
