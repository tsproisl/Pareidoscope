#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import os
import sqlite3
import sys
import tempfile

import networkx
from networkx.algorithms import isomorphism
from networkx.readwrite import json_graph

from pareidoscope import subgraph_enumeration
from pareidoscope.utils import cwb
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)
# logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.DEBUG)


def create_db(filename):
    """Create an empty database
    
    Arguments:
    - `filename`:
    """
    # SELECT sentid, position FROM tokens INNER JOIN types USING (typeid) INNER JOIN sentences USING (sentid) INNER JOIN indeps USING (typeid) INNER JOIN outdeps USING (typeid) WHERE â€¦
    conn = sqlite3.connect(filename)
    c = conn.cursor()
    c.execute("PRAGMA page_size=4096")
    c.execute("PRAGMA cache_size=100000")
    ## tables for finding vertice candidates
    # types
    c.execute("CREATE TABLE types (typeid INTEGER PRIMARY KEY AUTOINCREMENT, word TEXT, pos TEXT, lemma TEXT, wc TEXT, root INTEGER, indeg INTEGER, outdeg INTEGER)")
    c.execute("CREATE INDEX types_word_idx ON types (word)")
    c.execute("CREATE INDEX types_pos_idx ON types (pos)")
    c.execute("CREATE INDEX types_lemma_idx ON types (lemma)")
    c.execute("CREATE INDEX types_wc_idx ON types (wc)")
    c.execute("CREATE INDEX types_root_idx ON types (root)")
    c.execute("CREATE INDEX types_indeg_idx ON types (indeg)")
    c.execute("CREATE INDEX types_outdeg_idx ON types (outdeg)")
    # indeps
    c.execute("CREATE TABLE indeps (typeid INTEGER, indep TEXT, FOREIGN KEY (typeid) REFERENCES types, UNIQUE (typeid, indep))")
    c.execute("CREATE INDEX indeps_idx ON indeps (indep)")
    # outdeps
    c.execute("CREATE TABLE outdeps (typeid INTEGER, outdep TEXT, FOREIGN KEY (typeid) REFERENCES types, UNIQUE (typeid, outdep))")
    c.execute("CREATE INDEX outdeps_idx ON outdeps (outdep)")
    # sentences
    c.execute("CREATE TABLE sentences (sentid INTEGER PRIMARY KEY AUTOINCREMENT, origid TEXT, graph TEXT)")
    # tokens
    c.execute("CREATE TABLE tokens (typeid INTEGER, sentid INTEGER, position INTEGER, FOREIGN KEY (typeid) REFERENCES types, FOREIGN KEY (sentid) REFERENCES sentences, UNIQUE (typeid, sentid, position))")
    ## tables for stars
    c.execute("CREATE TABLE edge_stars (starid INTEGER PRIMARY KEY AUTOINCREMENT, subgraph TEXT, center INTEGER, length INTEGER, degree_sequence TEXT, edge_list TEXT, stars INTEGER, star_centers INTEGER, UNIQUE (subgraph, center))")
    c.execute("CREATE INDEX edegseq_idx ON edge_stars (degree_sequence)")
    c.execute("CREATE INDEX edge_list_idx ON edge_stars (edge_list)")
    c.execute("CREATE TABLE skel_stars (starid INTEGER PRIMARY KEY AUTOINCREMENT, subgraph TEXT, center INTEGER, length INTEGER, degree_sequence TEXT, stars INTEGER, star_centers INTEGER, UNIQUE (subgraph, center))")
    c.execute("CREATE INDEX sdegseq_idx ON skel_stars (degree_sequence)")
    # equivalence classes of isomorphic stars
    c.execute("CREATE TABLE edge_star_classes (classid INTEGER PRIMARY KEY AUTOINCREMENT, stars INTEGER, star_centers INTEGER)")
    c.execute("CREATE TABLE skel_star_classes (classid INTEGER PRIMARY KEY AUTOINCREMENT, stars INTEGER, star_centers INTEGER)")
    c.execute("CREATE TABLE edge_star_elements (classid INTEGER, elemid INTEGER, FOREIGN KEY (classid) REFERENCES edge_star_classes, FOREIGN KEY (elemid) REFERENCES edge_stars (starid), UNIQUE (elemid, classid))")
    c.execute("CREATE TABLE skel_star_elements (classid INTEGER, elemid INTEGER, FOREIGN KEY (classid) REFERENCES skel_star_classes, FOREIGN KEY (elemid) REFERENCES skel_stars (starid), UNIQUE (elemid, classid))")
    return conn, c


def tupleize(stuple):
    """Return a list of tuples representing the tokens
    
    Arguments:
    - `sentence`:
    """
    sentence, origid = stuple
    gs = nx_graph.create_nx_digraph_from_cwb(sentence, origid)
    sensible = nx_graph.is_sensible_graph(gs)
    result = {}
    result["tokens"] = {}
    result["graph"] = json.dumps(json_graph.node_link_data(gs), ensure_ascii=False)
    result["origid"] = origid
    for v in gs.nodes():
        word = gs.node[v]["word"]
        pos = gs.node[v]["pos"]
        lemma = gs.node[v]["lemma"]
        wc = gs.node[v]["wc"]
        root = 1 if "root" in gs.node[v] else 0
        indegree = gs.in_degree(v)
        outdegree = gs.out_degree(v)
        indeps = set([gs.edge[s][t]["relation"] for s, t in gs.in_edges(v)])
        indeps = tuple(sorted(list(indeps)))
        outdeps = set([gs.edge[s][t]["relation"] for s, t in gs.out_edges(v)])
        outdeps = tuple(sorted(list(outdeps)))
        t = (word, pos, lemma, wc, root, indegree, outdegree, indeps, outdeps)
        result["tokens"][v] = t
    return result, sensible


def preprocess_results(conn, c, result, results):
    """Collect results
    
    Arguments:
    - `result`:
    - `results`:
    """
    c.execute("INSERT INTO sentences (origid, graph) VALUES (?, ?)", (result["origid"], result["graph"],))
    sentid = c.lastrowid
    for position, t in result["tokens"].iteritems():
        if t not in results:
            results[t] = []
        results[t].append([sentid, position])


def insert_tokens(conn, c, results):
    """Insert token-level data into database
    
    Arguments:
    - `conn`:
    - `c`:
    - `results`:
    """
    for t in results:
        c.execute("INSERT INTO types (word, pos, lemma, wc, root, indeg, outdeg) VALUES (?,?,?,?,?,?,?)", t[0:7])
        typeid = c.lastrowid
        indeps = itertools.izip(itertools.repeat(typeid), t[7])
        c.executemany("INSERT INTO indeps VALUES (?,?)", indeps)
        outdeps = itertools.izip(itertools.repeat(typeid), t[8])
        c.executemany("INSERT INTO outdeps VALUES (?,?)", outdeps)
        tokens = itertools.izip(itertools.repeat([typeid]), results[t])
        tokens = (tuple(itertools.chain.from_iterable(token)) for token in tokens)
        c.executemany("INSERT INTO tokens VALUES (?,?,?)", tokens)
    conn.commit()


def extract_stars(args):
    """Extract subgraphs from sentence
    
    Arguments:
    - `sentid`:
    - `graph`:
    """
    sentid, graph = args
    edge_stars = collections.Counter()
    skel_stars = collections.Counter()
    gs = json_graph.node_link_graph(json.loads(graph))
    bfo_graph, bfo_to_raw = subgraph_enumeration.get_bfo(gs)
    bfo_to_bfo = {v: v for v in bfo_graph.nodes()}
    for vertice in bfo_graph.nodes():
        whole_star = bfo_graph.subgraph(set([vertice] + bfo_graph.predecessors(vertice) + bfo_graph.successors(vertice)))
        for subgraph in subgraph_enumeration.enumerate_csg_minmax(whole_star, bfo_to_bfo, min_vertices=2, max_vertices=whole_star.__len__()):
            # vertice must be in subgraph
            if not subgraph.has_node(vertice):
                continue
            # vertice must be center of star
            other_vertices = set(subgraph.nodes()) - set([vertice])
            if len(other_vertices) == 0:
                continue
            if not all((subgraph.has_edge(vertice, x) or subgraph.has_edge(x, vertice) for x in other_vertices)):
                continue
            length = subgraph.__len__()
            degree_sequence = " ".join([str(_) for _ in sorted([subgraph.degree(_) for _ in subgraph.nodes()], reverse=True)])
            sorted_edges = " ".join(sorted([l["relation"] for s, t, l in subgraph.edges(data=True)]))
            edge_star = subgraph.copy()
            nx_graph.skeletize_inplace(edge_star, only_vertices=True)
            edge_star, order = nx_graph.canonize(edge_star, order=True)
            edge_center = order.index(vertice)
            edge_t = tuple(networkx.generate_edgelist(edge_star, data=["relation"]))
            edge_stars[(edge_t, vertice, length, edge_center, degree_sequence, sorted_edges)] += 1
            skel_star = subgraph.copy()
            nx_graph.skeletize_inplace(skel_star)
            skel_star, order = nx_graph.canonize(skel_star, order=True)
            skel_center = order.index(vertice)
            skel_t = tuple(networkx.generate_edgelist(skel_star))
            skel_stars[(skel_t, vertice, length, skel_center, degree_sequence)] += 1
    return sentid, edge_stars, skel_stars


def _check_isomorphisms(star, graph, stars, varstr, candidates=[]):
    """Check if star is isomorphic to any known subgraph and if one of the
       isomorphisms maps the star centers to each other.

    """
    t_g_iterator = stars[varstr]["graph"].iteritems()
    if len(candidates) > 0:
        t_g_iterator = ((c, stars[varstr]["graph"][c]) for c in candidates)
    isomorphics = [t for t, g in t_g_iterator if networkx.is_isomorphic(g, graph, edge_match=lambda x, y: x.get("relation", "") == y.get("relation", ""))]
    for isomorphic in isomorphics:
        dgm = isomorphism.DiGraphMatcher(graph, stars[varstr]["graph"][isomorphic], edge_match=lambda x, y: x.get("relation", "") == y.get("relation", ""))
        if any(iso[star[1]] == isomorphic[1] for iso in dgm.isomorphisms_iter()):
            stars[varstr]["id"][star] = stars[varstr]["id"][isomorphic]
            logging.debug("%s is isomorphic to %s" % (star, isomorphic))
            return
    


def insert_stars(c, result):
    """Insert subgraphs into database
    
    Arguments:
    - `c`:
    - `result`:

    """
    sentid, edge_stars, skel_stars = result
    for star, freq in edge_stars.iteritems():
        graph_tuple, center, length, center, degree_sequence, sorted_edges = star
        graph_string = json.dumps(list(graph_tuple), ensure_ascii=False)
        c.execute("INSERT OR IGNORE INTO edge_stars (subgraph, center, length, degree_sequence, edge_list, stars, star_centers) VALUES (?, ?, ?, ?, ?, 0, 0)", (graph_string, center, length, degree_sequence, sorted_edges))
        c.execute("UPDATE edge_stars SET stars = stars + ?, star_centers = star_centers + ? WHERE subgraph = ?", (freq, 1, graph_string))
    for star, freq in skel_stars.iteritems():
        graph_tuple, center, length, center, degree_sequence = star
        graph_string = json.dumps(list(graph_tuple), ensure_ascii=False)
        c.execute("INSERT OR IGNORE INTO skel_stars (subgraph, center, length, degree_sequence, stars, star_centers) VALUES (?, ?, ?, ?, 0, 0)", (graph_string, center, length, degree_sequence))
        c.execute("UPDATE skel_stars SET stars = stars + ?, star_centers = star_centers + ? WHERE subgraph = ?", (freq, 1, graph_string))


def iso_candidates_iter(conn, is_edge_star):
    """Yield a list of potentially isomorphic subgraphs.

    Arguments:
    - `conn`:
    - `c`:
    - `is_edge_star`:

    """
    c1 = conn.cursor()
    q1 = "SELECT DISTINCT degree_sequence FROM skel_stars"
    q2 = "SELECT starid, subgraph, center, stars, star_centers FROM skel_stars WHERE degree_sequence=?"
    if is_edge_star:
        q1 = "SELECT DISTINCT degree_sequence, edge_list FROM edge_stars"
        q2 = "SELECT starid, subgraph, center, stars, star_centers FROM edge_stars WHERE degree_sequence=? AND edge_list=?"
    for row in c1.execute(q1):
        c2 = conn.cursor()
        yield c2.execute(q2, row).fetchall()


def collapse_isomorphisms(args):
    """"""
    candidates, is_edge_star = args
    print candidates
    #starid, subgraph, center, stars, star_centers = candidate
    sets = {}
    frequencies = {}
    done = [False for _ in candidates]
    subgraph = None
    if is_edge_star:
        subgraph = [networkx.parse_edgelist(json.loads(c[1]), nodetype=int, create_using=networkx.DiGraph(), data=(("relation", str),)) for c in candidates]
    else:
        subgraph = [networkx.parse_edgelist(json.loads(c[1]), nodetype=int, create_using=networkx.DiGraph()) for c in candidates]
    for i in range(len(candidates)):
        if done[i]:
            continue
        sets[i] = set([candidates[i][0]])
        frequencies[i] = {"stars": candidates[i][3], "star_centers": candidates[i][4]}
        done[i] = True
        for j in range(len(candidates)):
            if done[j]:
                continue
            dgm = isomorphism.DiGraphMatcher(subgraph[i], subgraph[j], edge_match=lambda x, y: x.get("relation", "") == y.get("relation", ""))
            if any(iso[int(candidates[i][2])] == int(candidates[j][2]) for iso in dgm.isomorphisms_iter()):
                sets[i].add(candidates[j][0])
                frequencies[i]["stars"] += candidates[j][3]
                frequencies[i]["star_centers"] += candidates[j][4]
                done[j] = True
                logging.debug("%s is isomorphic to %s" % (candidates[i][1], candidates[j][1]))
    return sets, frequencies


def insert_equivalence_classes(c, sets, frequencies, is_edge_star):
    """Insert equivalence classes of isomorphic stars into the database.

    """
    q1 = "INSERT INTO skel_star_classes (stars, star_centers) VALUES (?, ?)"
    q2 = "INSERT INTO skel_star_elements (classid, elemid) VALUES (?, ?)"
    if is_edge_star:
        q1 = "INSERT INTO edge_star_classes (stars, star_centers) VALUES (?, ?)"
        q2 = "INSERT INTO edge_star_elements (classid, elemid) VALUES (?, ?)"
    for i in sorted(sets.keys()):
        s = sets[i]
        stars = frequencies[i]["stars"]
        star_centers = frequencies[i]["star_centers"]
        c.execute(q1, (stars, star_centers))
        classid = c.lastrowid
        c.executemany(q2, zip(itertools.repeat(classid), s))



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Convert a corpus in CWB format to an SQLite database and collect frequency information about stars')
    parser.add_argument("--db", type=os.path.abspath, required=True, help="SQLite3 database")
    parser.add_argument("CORPUS", type=argparse.FileType("r"), help="Corpus in CWB format")
    args = parser.parse_args()

    # groupsize = 50 * 10 * multiprocessing.cpu_count()
    # # raise exeception if args.db already exists
    # if os.path.exists(args.db):
    #     raise Exception("Database file already exists")
    # conn, c = create_db(args.db)
    # pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    # sents = cwb.sentences_iter(args.CORPUS, return_id=True)
    # results = {}
    # logging.info("tupleize and preprocess")
    # for sentences in helper.grouper_nofill(groupsize, sents):
    #     r = pool.imap_unordered(tupleize, sentences, 10)
    #     # r = map(tupleize, sentences)
    #     for result, sensible in r:
    #         if sensible:
    #             preprocess_results(conn, c, result, results)
    # logging.info("insert tokens")
    # insert_tokens(conn, c, results)
    # logging.info("exctract stars")
    # sent_count = c.execute("SELECT count(*) FROM sentences").fetchone()[0]
    # logging.info("%d sentences" % sent_count)
    # for offset in xrange(0, sent_count, groupsize):
    #     sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
    #     logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
    #     r = pool.imap_unordered(extract_stars, sentences, 10)
    #     # r = map(extract_stars, sentences)
    #     for result in r:
    #         insert_stars(c, result)
    #     conn.commit()
    # conn = sqlite3.connect(args.db)
    # c = conn.cursor()
    # logging.info("collapse isomorphic stars")
    conn = sqlite3.connect(args.db)
    c = conn.cursor()
    #for is_edge_star in [False, True]:
    for is_edge_star in [True]:
        logging.info("... %s" % ("edge stars" if is_edge_star else "skel_stars"))
        cands = iso_candidates_iter(conn, is_edge_star)
        for candidates in helper.grouper_nofill(groupsize, cands):
            # r = pool.imap_unordered(collapse_isomorphisms, itertools.izip(candidates, itertools.repeat(is_edge_star)), 10)
            r = map(collapse_isomorphisms, itertools.izip(candidates, itertools.repeat(is_edge_star)))
            for result in r:
                print result
                sets, frequencies = result
                insert_equivalence_classes(c, sets, frequencies, is_edge_star)
    conn.commit()
    conn.close()
    logging.info("done")
