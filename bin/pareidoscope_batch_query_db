#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import itertools
import json
import logging
import math
import multiprocessing
import os
import sqlite3
import subprocess
import sys

from networkx.readwrite import json_graph

from pareidoscope import subgraph_isomorphism
from pareidoscope import query
from pareidoscope.utils import cwb
from pareidoscope.utils import database
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def read_queries(queries_file):
    """Read all queries.
    
    Arguments:
    - `queries_file`:
    """
    queries = [line.decode("utf-8").rstrip("\n").split("\t") for line in queries_file]
    queries = [[nx_graph.create_nx_digraph(json.loads(q)) for q in l] for l in queries]
    return queries


def check_tables(c):
    """Check tables in database and return if isomorphisms have been
    collected.
    
    Arguments:
    - `c`:

    """
    q = "SELECT name FROM sqlite_master WHERE type=? AND name=?"
    subgraphs = c.execute(q, ("table", "subgraphs")).fetchall() != []
    return subgraphs


def write_results(prefix, results):
    """Write results to files
    
    Arguments:
    - `prefix`:
    - `results`:
    """
    with open("%s.dat" % prefix, "w") as fh:
        json.dump(results, fh)
    for suffix in results[0].keys():
        with open("%s_%s.mi" % (prefix, suffix), "w") as fh:
            fh.write("MI\n")
            for i, q in enumerate(results):
                s = q[suffix]
                e11 = 0
                if s["o11"] == 0:
                    continue
                if suffix in ["iso_ct", "sub_ct"]:
                    if any([s[x] == 0 for x in ["r1", "c1", "n"]]):
                        e11 = 0
                    else:
                        e11 = (s["r1"] * s["c1"]) / float(s["n"])
                elif suffix == "sub_bnl":
                    if any([s[x] == 0 for x in s.keys()]):
                        e11 = 0
                    else:
                        # po11 = sum([math.log(x) for x in [s["a"]/float(s["n"]), s["b"]/float(s["n"]), s["r1"]/float(s["a"]), s["c1"]/float(s["b"]), s["o11"]/float(s["r1+c1"])]])
                        po11 = sum([math.log(x) for x in [s["a"]/float(s["n"]), s["b"]/float(s["n"]), s["r1"]/float(s["a"]), s["c1"]/float(s["b"])]])
                        e11 = (math.e ** po11) * s["n"]
                elif suffix == "sub_bns":
                    if any([s[x] == 0 for x in s.keys()]):
                        e11 = 0
                    else:
                        # po11 = sum([math.log(x) for x in [s["a"]/float(s["n"]), s["b"]/float(s["n"]), s["o11"]/float(s["a+b"])]])
                        po11 = sum([math.log(x) for x in [s["a"]/float(s["n"]), s["b"]/float(s["n"])]])
                        e11 = (math.e ** po11) * s["n"]
                elif suffix == "sent_bnl":
                    if any([s[x] == 0 for x in s.keys()]):
                        e11 = 0
                    else:
                        # po11 = sum([math.log(x) for x in [s["a"]/float(s["size"]), s["n"]/float(s["size"]), s["b"]/float(s["size"]), s["r1"]/float(s["a+n"]), s["c1"]/float(s["b+n"]), s["o11"]/float(s["r1+c1"])]])
                        po11 = sum([math.log(x) for x in [s["a"]/float(s["size"]), s["n"]/float(s["size"]), s["b"]/float(s["size"]), s["r1"]/float(s["a+n"]), s["c1"]/float(s["b+n"])]])
                        e11 = (math.e ** po11) * s["size"]
                elif suffix == "sent_bns":
                    if any([s[x] == 0 for x in s.keys()]):
                        e11 = 0
                    else:
                        # po11 = sum([math.log(x) for x in [s["a"]/float(s["size"]), s["n"]/float(s["size"]), s["b"]/float(s["size"]), s["o11"]/float(s["a+b+n"])]])
                        po11 = sum([math.log(x) for x in [s["a"]/float(s["size"]), s["n"]/float(s["size"]), s["b"]/float(s["size"])]])
                        e11 = (math.e ** po11) * s["size"]
                mi = math.log(s["o11"]/float(e11), 2)
                fh.write("%d\t%f\n" % (i+1, mi))
        script_directory = os.path.abspath(os.path.dirname(sys.argv[0]))
        working_directory = os.path.abspath(os.getcwd())
        subprocess.call([script_directory+"/analyze_batch_queries.R", "%s/%s_%s.mi" % (working_directory, prefix, suffix), suffix])



if __name__ == "__main__":
    if sys.version_info < (2, 7):
        raise Exception("Need at least Python 2.7!")
    os.nice(10)
    parser = argparse.ArgumentParser(description='Run a batch of queries against a corpus')
    parser.add_argument("--db", type=os.path.abspath, required=True, help="Corpus database")
    parser.add_argument("-o", "--output", type=str, required=True, help="Output prefix")
    parser.add_argument("QUERIES", type=argparse.FileType("r"), help="Queries for O11, A, B, R1, C1 and N")
    args = parser.parse_args()
    
    groupsize = 5 * 100 * multiprocessing.cpu_count()
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    logging.info("connect to database '%s'" % args.db)
    conn = sqlite3.connect(args.db)
    c = conn.cursor()
    c.execute("PRAGMA cache_size=100000")
    subgraphs = check_tables(c)
    conn.create_function("REGEXP", 2, database.regexp)
    logging.info("read query file")
    queries = read_queries(args.QUERIES)
    results = [{} for q in queries]
    logging.info("%d queries", len(queries))
    for query_number, qline in enumerate(queries):
        logging.info("process query %d", query_number+1)
        go11, ga, gb, gr1, gc1, gn = qline
        candidates = None
        if nx_graph.is_purely_structural(gn) and subgraphs:
            logging.info("get structure candidates")
            candidates = database.get_structure_candidates(c, gn)
        else:
            logging.info("get candidates")
            candidates = database.get_candidates(c, gn)
        logging.info("%d candidate sentences", len(candidates))
        for sentids in helper.grouper_nofill(groupsize, candidates):
            logging.info("process %d sentences", len(sentids))
            q = "SELECT graph FROM sentences WHERE sentid=?"
            sentences = [c.execute(q, (sentid,)).fetchone()[0] for sentid in sentids]
            cands = [candidates[sentid] for sentid in sentids]
            arguments = itertools.izip(itertools.repeat(go11), itertools.repeat(ga), itertools.repeat(gb), itertools.repeat(gr1), itertools.repeat(gc1), itertools.repeat(gn), sentences, cands)
            r = pool.imap(query.run_queries_db, arguments, 10)
            # r = map(query.run_queries_db, args)
            for result in r:
                query.merge_result_db(result, query_number, results)
    logging.info("write results")
    write_results(args.output, results)
