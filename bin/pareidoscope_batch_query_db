#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import functools
import itertools
import json
import logging
import multiprocessing
import os

from networkx.readwrite import json_graph

import pareidoscope.query
from pareidoscope.utils import database
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def arguments():
    """"""
    parser = argparse.ArgumentParser(description='Run a batch of queries against a corpus')
    parser.add_argument("-c", "--corpus", type=os.path.abspath, required=True, help="SQLite3 corpus database as created by pareidoscope_cwb_to_sqlite")
    parser.add_argument("-o", "--output", type=str, required=True, help="Output prefix")
    parser.add_argument("-p", "--cpu", type=int, default=25, help="Percentage of CPUs to use (0-100; default: 25)")
    parser.add_argument("QUERIES", type=argparse.FileType("r", encoding="utf-8"), help="Queries file with one graph per line and vertices marked as belonging to A, B or both (AB)")
    return parser.parse_args()


def read_queries(queries_file):
    """Read all queries."""
    for line in queries_file:
        if line.startswith("#") or line == "\n":
            continue
        yield json_graph.node_link_graph(json.loads(line))


def extract_graphs(query):
    """Extract query graphs for A, B and N"""
    a = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "A" or l["query"] == "AB"])
    b = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "B" or l["query"] == "AB"])
    n = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "AB"])
    for v, l in query.nodes(data=True):
        l["vid"] = v
        del l["query"]
    return [nx_graph.ensure_consecutive_vertices(g) for g in (query, a, b, n)]


def add_focus_point(query):
    """Search for choke point vertices"""
    focus_point_vertex = nx_graph.get_choke_point(query[3])
    # focus_point_vid = query[3].node[focus_point_vertex]["vid"]
    return query + [focus_point_vertex]


def sentence_candidates(c, g):
    """Get candidate sentences (no token candidates!) for the query
    graph.

    """
    sentence_ids = []
    for vertex in g.nodes():
        sql_query = "SELECT sentence_id FROM tokens WHERE "
        where = []
        args = []
        pos_lexical = set(["word", "pos", "lemma", "wc", "root"])
        neg_lexical = set(["not_%s" % pl for pl in pos_lexical])
        indegree = g.in_degree(vertex)
        outdegree = g.out_degree(vertex)
        if indegree > 0:
            where.append("indegree >= ?")
            args.append(indegree)
        if outdegree > 0:
            where.append("outdegree >= ?")
            args.append(outdegree)
        for k, v in g.node[vertex].items():
            if k in pos_lexical:
                where.append("%s = ?" % k)
                if k == "root":
                    args.append(v == "root")
                else:
                    args.append(v)
            elif k in neg_lexical:
                where.append("%s != ?" % k)
                if k == "root":
                    args.append(v == "root")
                else:
                    args.append(v)
            elif k == "not_indep" or k == "not_outdep":
                pass
            else:
                raise Exception("Unsupported key: %s" % k)
        sql_query += " AND ".join(where)
        sentence_ids.append(set([r[0] for r in c.execute(sql_query, args).fetchall()]))
    candidate_ids = functools.reduce(lambda x, y: x.intersection(y), sentence_ids)
    candidate_sentences = [r[0] for r in c.execute("SELECT graph FROM sentences WHERE sentence_id IN (%s)" % ", ".join([str(_) for _ in candidate_ids])).fetchall()]
    return candidate_sentences


def write_results(prefix, results):
    """Write results to files

    Arguments:
    - `prefix`:
    - `results`:
    """
    with open("%s.dat" % prefix, "w") as fh:
        json.dump(results, fh)
    # for suffix in results[0].keys():
    #     with open("%s_%s.mi" % (prefix, suffix), "w") as fh:
    #         fh.write("MI\n")
    #         for i, q in enumerate(results):
    #             s = q[suffix]
    #             e11 = 0
    #             if s["o11"] == 0:
    #                 continue
    #             if suffix in ["iso_ct", "sub_ct", "choke_point_ct", "sent_ct"]:
    #                 if any([s[x] == 0 for x in ["r1", "c1", "n"]]):
    #                     e11 = 0
    #                 else:
    #                     e11 = (s["r1"] * s["c1"]) / float(s["n"])
    #             else:
    #                 pass
    #             mi = math.log(s["o11"]/float(e11), 2)
    #             fh.write("%d\t%f\n" % (i+1, mi))


def main():
    """"""
    args = arguments()
    queries = read_queries(args.QUERIES)
    queries = [extract_graphs(q) for q in queries]
    queries = [add_focus_point(q) for q in queries]
    # print(queries)
    # print([[json.dumps(json_graph.node_link_data(g)) for g in q[:3]] for q in queries])

    conn, c = database.connect_to_database(args.corpus)

    groupsize = 10 * 10 * multiprocessing.cpu_count()

    results = [{} for q in queries]
    cpu_count = multiprocessing.cpu_count()
    processes = min(max(1, int(cpu_count * args.cpu / 100)), cpu_count)
    pool = multiprocessing.Pool(processes=processes)
    # with multiprocessing.Pool(processes=processes) as pool:
    for i, qline in enumerate(queries):
        logging.info("query no. %d" % i)
        gc, ga, gb, gn, choke_point = qline
        # print(json.dumps(json_graph.node_link_data(gn)))
        # find candidates for gn
        # query_sql, args_sql = database.create_sql_query(pareidoscope.query.strip_vid(gn))
        # sents_and_candidates = database.aggregate_sentences(c, query_sql, args_sql)
        sents = sentence_candidates(c, pareidoscope.query.strip_vid(gn))
        logging.info("%d sentence candidates" % len(sents))
        # for sentences in helper.grouper_nofill(groupsize, sents_and_candidates):
        for sentences in helper.grouper_nofill(groupsize, sents):
            # remove sentence ids
            # sentences = [s[1:] for s in sentences]
            # query_args = (itertools.chain.from_iterable(a) for a in zip(itertools.repeat(qline), sentences))
            query_args = (itertools.chain.from_iterable(a) for a in zip(itertools.repeat(qline), ((s,) for s in sentences), ((t,) for t in itertools.repeat(True))))
            r = pool.imap_unordered(pareidoscope.query.run_queries_db, query_args, 10)
            # r = map(pareidoscope.query.run_queries_db, query_args)
            for result in r:
                pareidoscope.query.merge_result_db(result, i, results)
        logging.info(results[i])
    write_results(args.output, results)


if __name__ == "__main__":
    main()
