#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import itertools
import json
import logging
import multiprocessing
import os
import re
import sqlite3

from networkx.readwrite import json_graph
import numpy
import scipy.stats

from pareidoscope import frequency
from pareidoscope import subgraph_enumeration
from pareidoscope import subgraph_isomorphism
from pareidoscope.utils import collexeme_database
from pareidoscope.utils import cwb
from pareidoscope.utils import database
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph
from pareidoscope.utils import statistics

# logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)
logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.DEBUG)


def arguments():
    """"""
    parser = argparse.ArgumentParser(description="Do a collexeme analysis for the given linguistic structure, i.e. find associated lemmata for a specified slot in the structure")
    parser.add_argument("-c", "--corpus", type=os.path.abspath, required=True, help="SQLite3 corpus database as created by pareidoscope_cwb_to_sqlite")
    parser.add_argument("--db", type=os.path.abspath, required=True, help="SQLite3 database for results")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("-w", "--wordforms", help="Find collexeme wordforms", action='store_true')
    group.add_argument("-l", "--lemmata", help="Find collexeme lemmata", action='store_true')
    parser.add_argument("QUERY", type=str, help="Query for a linguistic structure")
    parser.add_argument("INDEX", type=int, help="Index of the collexeme slot")
    parser.add_argument("WC", type=str, help="Word class of the collexeme slot")
    args = parser.parse_args()
    return args


def collect_frequencies(args):
    """Collect all necessary frequencies"""
    s, query, query_skeleton, choke_point, collexeme_index, lemma, wc = args
    sid, sentence, candidates = s
    # * frequencies we need
    # ** collostructional
    # - lemma in graph (centers)
    # - lemma (-> db)
    # - word class in graph
    # - word class (-> db)
    # ** other
    # - lemma in graph
    # - lemma in skeleton
    # - graph
    # - skeleton
    gs = json_graph.node_link_graph(json.loads(sentence))
    sensible = nx_graph.is_sensible_graph(gs)
    o11 = {"collostructional": {}, "choke_points": {}, "subgraphs": {}, "isomorphisms": {}}
    r1 = {"collostructional": {}, "choke_points": {}, "subgraphs": {}, "isomorphisms": {}}
    c1 = {"collostructional": 0, "choke_points": 0, "subgraphs": 0, "isomorphisms": 0}
    n = {"collostructional": 0, "choke_points": 0, "subgraphs": 0, "isomorphisms": 0}
    choke_point_query_set = set()
    choke_point_skel_set = set()
    isomorphisms_query_set = set()
    isomorphisms_skel_set = set()
    sur_attribute = "word"
    if lemma:
        sur_attribute = "lemma"
    if sensible:
        for isomorphism in subgraph_isomorphism.get_subgraph_isomorphisms_nx(query_skeleton, gs, vertice_candidates=candidates):
            n["isomorphisms"] += 1
            choke_point_skel_set.add(isomorphism[choke_point])
            isomorphisms_skel_set.add(frozenset(isomorphism))
            collexeme = gs.node[isomorphism[collexeme_index]][sur_attribute]
            if collexeme not in r1["isomorphisms"]:
                r1["choke_points"][collexeme] = 0
                r1["subgraphs"][collexeme] = 0
                r1["isomorphisms"][collexeme] = 0
            r1["isomorphisms"][collexeme] += 1
        n["choke_points"] += len(choke_point_skel_set)
        n["subgraphs"] += len(isomorphisms_skel_set)
        for collexeme in r1["isomorphisms"].keys():
            lexical_skel = query_skeleton.copy()
            lexical_skel.node[collexeme_index][sur_attribute] = re.escape(collexeme)
            cands = nx_graph.get_vertice_candidates(lexical_skel, gs)
            lexical_candidates = [c & cands[i] for i, c in enumerate(candidates)]
            r1["subgraphs"][collexeme] += sum(1 for _ in subgraph_enumeration.get_subgraphs_nx(lexical_skel, gs, vertice_candidates=lexical_candidates))
            r1["choke_points"][collexeme] += sum(1 for _ in subgraph_enumeration.get_choke_point_matches(lexical_skel, gs, choke_point, vertice_candidates=lexical_candidates))
        cands = nx_graph.get_vertice_candidates(query, gs)
        query_candidates = [c & cands[i] for i, c in enumerate(candidates)]
        for isomorphism in subgraph_isomorphism.get_subgraph_isomorphisms_nx(query, gs, vertice_candidates=query_candidates):
            c1["isomorphisms"] += 1
            choke_point_query_set.add(isomorphism[choke_point])
            isomorphisms_query_set.add(frozenset(isomorphism))
            collexeme = gs.node[isomorphism[collexeme_index]][sur_attribute]
            if collexeme not in o11["isomorphisms"]:
                o11["collostructional"][collexeme] = 0
                o11["choke_points"][collexeme] = 0
                o11["subgraphs"][collexeme] = 0
                o11["isomorphisms"][collexeme] = 0
            o11["isomorphisms"][collexeme] += 1
        c1["choke_points"] += len(choke_point_query_set)
        c1["subgraphs"] += len(isomorphisms_query_set)
        for collexeme in o11["isomorphisms"].keys():
            lexical_query = query.copy()
            lexical_query.node[collexeme_index][sur_attribute] = re.escape(collexeme)
            cands = nx_graph.get_vertice_candidates(lexical_query, gs)
            lexical_candidates = [c & cands[i] for i, c in enumerate(candidates)]
            o11["subgraphs"][collexeme] += sum(1 for _ in subgraph_enumeration.get_subgraphs_nx(lexical_query, gs, vertice_candidates=lexical_candidates))
            o11["choke_points"][collexeme] += sum(1 for _ in subgraph_enumeration.get_choke_point_matches(lexical_query, gs, choke_point, vertice_candidates=lexical_candidates))
        # collostructional
        o11["collostructional"] = o11["choke_points"]
        wc_query = query.copy()
        wc_query.node[collexeme_index]["wc"] = wc
        cands = nx_graph.get_vertice_candidates(wc_query, gs)
        wc_candidates = [c & cands[i] for i, c in enumerate(candidates)]
        c1["collostructional"] = sum(1 for _ in subgraph_enumeration.get_choke_point_matches(wc_query, gs, choke_point, vertice_candidates=wc_candidates))
    return o11, r1, c1, n


def calculate_associations(o11, r1, c1, n, measure=statistics.one_sided_log_likelihood):
    """"""
    associations = {}
    for count_method in o11.keys():
        f_c1 = c1[count_method]
        f_n = n[count_method]
        for collexeme in o11[count_method]:
            if collexeme not in associations:
                associations[collexeme] = {}
            f_o11 = o11[count_method][collexeme]
            f_r1 = r1[count_method][collexeme]
            o, e = statistics.get_contingency_table(f_o11, f_r1, f_c1, f_n)
            associations[collexeme][count_method] = measure(o, e)
    return associations


def calculate_association_strengths_and_correlations(o11, r1, c1, n):
    """"""
    dice_associations = calculate_associations(o11, r1, c1, n, statistics.dice)
    gmean_associations = calculate_associations(o11, r1, c1, n, statistics.geometric_mean)
    tscore_associations = calculate_associations(o11, r1, c1, n, statistics.t_score)
    ll_associations = calculate_associations(o11, r1, c1, n, statistics.one_sided_log_likelihood)
    if len(dice_associations) > 1:
        dice_correlations = scipy.stats.spearmanr([[v["collostructional"], v["choke_points"], v["subgraphs"], v["isomorphisms"]] for v in dice_associations.itervalues()])[0]
        gmean_correlations = scipy.stats.spearmanr([[v["collostructional"], v["choke_points"], v["subgraphs"], v["isomorphisms"]] for v in gmean_associations.itervalues()])[0]
        tscore_correlations = scipy.stats.spearmanr([[v["collostructional"], v["choke_points"], v["subgraphs"], v["isomorphisms"]] for v in tscore_associations.itervalues()])[0]
        ll_correlations = scipy.stats.spearmanr([[v["collostructional"], v["choke_points"], v["subgraphs"], v["isomorphisms"]] for v in ll_associations.itervalues()])[0]
    else:
        dice_correlations = numpy.ones((4,4))
        gmean_correlations = numpy.ones((4,4))
        tscore_correlations = numpy.ones((4,4))
        ll_correlations = numpy.ones((4,4))
    return dice_associations, gmean_associations, tscore_associations, ll_associations, dice_correlations, gmean_correlations, tscore_correlations, ll_correlations


def aggregate_sentences(c, query, query_args):
    """Aggregate sentences, returning sentence id, sentence graph, and
    sets of candidates."""
    old_sentence_id = None
    candidates = None
    old_graph = None
    for row in c.execute(query, query_args):
        sentence_id = row[0]
        graph = row[1]
        token_positions = row[2:]
        if sentence_id != old_sentence_id and old_sentence_id is not None:
            yield old_sentence_id, old_graph, candidates
            candidates = None
        if candidates is None:
            candidates = [set([t]) for t in token_positions]
        else:
            for i, t in enumerate(token_positions):
                candidates[i].add(t)
        old_sentence_id = sentence_id
        old_graph = graph


def main():
    """"""
    args = arguments()
    query, order = nx_graph.canonize(nx_graph.create_nx_digraph(json.loads(args.QUERY.strip())), order=True)
    query_skeleton = nx_graph.skeletize(query)
    query_str = json.dumps(json_graph.node_link_data(query), ensure_ascii=False)
    query_skel_str = json.dumps(json_graph.node_link_data(query_skeleton), ensure_ascii=False)
    logging.debug("query graph: %s" % query_str)
    logging.debug("query skeleton: %s" % query_skel_str)
    choke_point = nx_graph.get_choke_point(query)
    logging.debug("choke point: %s" % choke_point)
    collexeme_index = order.index(args.INDEX)
    logging.debug("collexeme index: %s" % collexeme_index)
    lemma = args.lemmata
    if lemma:
        word_or_lemma = "lemma"
    logging.debug("collexeme as lemma: %s" % lemma)
    wc = args.WC
    logging.debug("collexeme word class: %s" % wc)
    query.node[collexeme_index]["wc"] = wc
    query_sql, args_sql = database.create_sql_query(query)
    logging.debug("SQL query: %s" % query_sql)
    query_skel_sql, args_skel_sql = database.create_sql_query(query_skeleton)
    logging.debug("SQL skeleton query: %s" % query_skel_sql)
    word_or_lemma = "word"

    groupsize = 50 * 10 * multiprocessing.cpu_count()

    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())
    # sents = cwb.sentences_iter(args.corpus, return_id=True)

    conn, c = database.connect_to_database(args.corpus)

    o11 = {"collostructional": {}, "choke_points": {}, "subgraphs": {}, "isomorphisms": {}}
    r1 = {"collostructional": {}, "choke_points": {}, "subgraphs": {}, "isomorphisms": {}}
    c1 = {"collostructional": 0, "choke_points": 0, "subgraphs": 0, "isomorphisms": 0}
    n = {"collostructional": 0, "choke_points": 0, "subgraphs": 0, "isomorphisms": 0}
    n["collostructional"] = c.execute("SELECT count(*) FROM tokens WHERE wc=?", (wc,)).fetchall()[0][0]

    logging.info("collect frequencies")
    sents_and_candidates = aggregate_sentences(c, query_skel_sql, args_skel_sql)
    for sentences in helper.grouper_nofill(groupsize, sents_and_candidates):
        r = pool.imap_unordered(collect_frequencies, itertools.izip(sentences, itertools.repeat(query), itertools.repeat(query_skeleton), itertools.repeat(choke_point), itertools.repeat(collexeme_index), itertools.repeat(lemma), itertools.repeat(wc)), chunksize=10)
        # r = map(collect_frequencies, itertools.izip(sentences, itertools.repeat(query), itertools.repeat(query_skeleton), itertools.repeat(choke_point), itertools.repeat(collexeme_index), itertools.repeat(lemma), itertools.repeat(wc)))
        for o11_local, r1_local, c1_local, n_local in r:
            frequency.merge_nested_result(o11_local, o11)
            frequency.merge_nested_result(r1_local, r1)
            frequency.merge_result(c1_local, c1)
            frequency.merge_result(n_local, n)
    for collexeme in o11["choke_points"]:
        r1["collostructional"][collexeme] = c.execute("SELECT count(*) FROM tokens WHERE %s=?" % word_or_lemma, (wc,)).fetchall()[0][0]
    logging.info("calculate association strengths and correlations")
    dice_associations, gmean_associations, tscore_associations, ll_associations, dice_correlations, gmean_correlations, tscore_correlations, ll_correlations = calculate_association_strengths_and_correlations(o11, r1, c1, n)
    logging.info("insert into database")
    collexeme_database.insert_results(args.db, query_str, lemma, collexeme_index, wc, o11, r1, c1, n, dice_associations, gmean_associations, tscore_associations, ll_associations, dice_correlations, gmean_correlations, tscore_correlations, ll_correlations)
    conn.close()
    logging.info("done")



if __name__ == "__main__":
    main()
