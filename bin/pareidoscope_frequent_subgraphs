#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import os
import sqlite3

import networkx
from networkx.readwrite import json_graph

from pareidoscope import subgraph_enumeration
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def connect_to_db(filename):
    """Connect to an existing database.
    
    Args:
      filename:

    """
    if not os.path.exists(filename):
        raise Exception("Database file does not exist")
    conn = sqlite3.connect(filename)
    c = conn.cursor()
    c.execute("PRAGMA page_size=4096")
    c.execute("PRAGMA cache_size=100000")
    return conn, c


def get_lemmata_and_edges(args):
    """Return the lemmata and edges of the graph.

    Args:
      args: (sentid, graph)

    """
    sentid, graph = args
    lemmata, edges = set(), set()
    gs = json_graph.node_link_graph(json.loads(graph))
    for v, l in gs.nodes(data=True):
        lemmata.update([l["lemma"]])
    for s, t, l in gs.edges(data=True):
        edges.update([l["relation"]])
    return lemmata, edges


def get_filtered_subgraphs(args):
    """Return subgraphs consisting of frequent vertices and edges.

    Args:
      args: ((sentid, graph), target_lemma, lemma_whitelist, edge_whitelist, min_vertices, max_vertices)

    """
    ((sentid, graph), target_lemma, lemma_whitelist, edge_whitelist, min_vertices, max_vertices) = args
    result = set()
    gs = json_graph.node_link_graph(json.loads(graph))
    if all(l["lemma"] != target_lemma for v, l in gs.nodes(data=True)):
        return result
    for v, l in gs.nodes(data=True):
        if l["lemma"] not in lemma_whitelist:
            gs.remove_node(v)
    for s, t, l in gs.edges(data=True):
        if l["relation"] not in edge_whitelist:
            gs.remove_edge(s, t)
    for comp in networkx.weakly_connected_component_subgraphs(gs):
        if comp.__len__() < min_vertices:
            continue
        if all(l["lemma"] != target_lemma for v, l in comp.nodes(data=True)):
            continue
        bfo_graph, bfo_to_raw = None, None
        try:
            bfo_graph, bfo_to_raw = subgraph_enumeration.get_bfo(comp, fragment=True)
        except IndexError:
            continue
        for subgraph in subgraph_enumeration.enumerate_csg_minmax(bfo_graph, bfo_to_raw, min_vertices=min_vertices, max_vertices=max_vertices):
            if all(l["lemma"] != target_lemma for v, l in subgraph.nodes(data=True)):
                continue
            subgraph = nx_graph.canonize(subgraph)
            result.update([json.dumps(json_graph.node_link_data(subgraph))])
    return result



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Extract frequent subgraphs')
    parser.add_argument("--min-subgraph", type=int, help="Minimum subgraph size (default: 2 vertices)", default=2, metavar="N")
    parser.add_argument("--max-subgraph", type=int, help="Maximum subgraph size (default: 5 vertices)", default=5, metavar="N")
    parser.add_argument("-s", "--support", type=int, help="Minimum support (default: 50 sentences)", default=50, metavar="N")
    parser.add_argument("-l", "--lemmafilter", type=str, required=True, help="Only subgraphs containing a specific lemma")
    parser.add_argument("-o", "--outfile", type=argparse.FileType("w"), required=True, help="Output file (JSON)")
    parser.add_argument("DB", type=os.path.abspath, help="SQLite3 database")
    args = parser.parse_args()

    groupsize = 50 * 10 * multiprocessing.cpu_count()
    conn, c = connect_to_db(args.DB)
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    sent_count = c.execute("SELECT count(*) FROM sentences").fetchone()[0]
    edges = collections.Counter()
    lemmata = collections.Counter()
    logging.info("%d sentences" % sent_count)
    logging.info("collect frequency information")
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        r = pool.imap_unordered(get_lemmata_and_edges, sentences, 10)
        # r = map(get_vertices_and_edges, sentences)
        for ls, es in r:
            for l in ls: lemmata[l] += 1
            for e in es: edges[e] += 1
    lemma_whitelist = set((l for l in lemmata if lemmata[l] >= args.support))
    edge_whitelist = set((e for e in edges if edges[e] >= args.support))
    if args.lemmafilter not in lemma_whitelist:
        logging.warning("Support for %s is lower than %d" % (args.lemmafilter, args.support))
        exit()
    subgraphs = collections.Counter()
    logging.info("find subgraphs with support >= %d" % args.support)
    sent_count = c.execute("SELECT count(*) FROM sentences INNER JOIN tokens USING (sentid) INNER JOIN types USING (typeid) WHERE types.lemma=?", (args.lemmafilter, )).fetchone()[0]
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT DISTINCT sentid, graph FROM sentences INNER JOIN tokens USING (sentid) INNER JOIN types USING (typeid) WHERE types.lemma=? LIMIT ? OFFSET ?", (args.lemmafilter, groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        arguments = itertools.izip(sentences, itertools.repeat(args.lemmafilter), itertools.repeat(lemma_whitelist), itertools.repeat(edge_whitelist), itertools.repeat(args.min_subgraph), itertools.repeat(args.max_subgraph))
        r = pool.imap_unordered(get_filtered_subgraphs, arguments, 10)
        # r = map(get_subgraphs, arguments)
        for sgs in r:
            for sg in sgs: subgraphs[sg] += 1
    logging.info("write output file")
    json.dump({k: v for k, v in subgraphs.iteritems() if v >= args.support}, args.outfile)
