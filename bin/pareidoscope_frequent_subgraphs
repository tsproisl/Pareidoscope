#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import os
import sqlite3

import networkx
from networkx.readwrite import json_graph

from pareidoscope import subgraph_enumeration
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def connect_to_db(filename):
    """Connect to an existing database.
    
    Args:
      filename:

    """
    if not os.path.exists(filename):
        raise Exception("Database file does not exist")
    conn = sqlite3.connect(filename)
    c = conn.cursor()
    c.execute("PRAGMA page_size=4096")
    c.execute("PRAGMA cache_size=100000")
    return conn, c


def get_vertices_and_edges(args):
    """Return the vertices and edges of the graph.

    Args:
      args: (sentid, graph)

    """
    sentid, graph = args
    vertices, edges = set(), set()
    gs = json_graph.node_link_graph(json.loads(graph))
    for vertice in gs.nodes():
        word = gs.node[vertice]["word"]
        pos = gs.node[vertice]["pos"]
        lemma = gs.node[vertice]["lemma"]
        wc = gs.node[vertice]["wc"]
        vertices.update([(word, pos, lemma, wc)])
    for s, t, l in gs.edges(data=True):
        edges.update([l["relation"]])
    return vertices, edges


def get_words_and_edges(args):
    """Return the words and edges of the graph.

    Args:
      args: (sentid, graph)

    """
    sentid, graph = args
    words, edges = set(), set()
    gs = json_graph.node_link_graph(json.loads(graph))
    for vertice in gs.nodes():
        word = gs.node[vertice]["word"]
        words.update([word])
    for s, t, l in gs.edges(data=True):
        edges.update([l["relation"]])
    return words, edges


def get_subgraphs(args):
    """Return subgraphs consisting of frequent vertices and edges.

    Args:
      args: ((sentid, graph), vertice_whitelist, edge_whitelist, min_vertices, max_vertices)

    """
    ((sentid, graph), vertice_whitelist, edge_whitelist, min_vertices, max_vertices) = args
    result = set()
    gs = json_graph.node_link_graph(json.loads(graph))
    for vertice in gs.nodes():
        word = gs.node[vertice]["word"]
        pos = gs.node[vertice]["pos"]
        lemma = gs.node[vertice]["lemma"]
        wc = gs.node[vertice]["wc"]
        if (word, pos, lemma, wc) not in vertice_whitelist:
            gs.remove_node(vertice)
    for s, t, l in gs.edges(data=True):
        if l["relation"] not in edge_whitelist:
            gs.remove_edge(s, t)
    for comp in networkx.weakly_connected_component_subgraphs(gs):
        if comp.__len__() < min_vertices:
            continue
        bfo_graph, bfo_to_raw = None, None
        try:
            bfo_graph, bfo_to_raw = subgraph_enumeration.get_bfo(comp, fragment=True)
        except IndexError:
            continue
        for subgraph in subgraph_enumeration.enumerate_csg_minmax(bfo_graph, bfo_to_raw, min_vertices=min_vertices, max_vertices=max_vertices):
            subgraph = nx_graph.canonize(subgraph)
            result.update([json.dumps(json_graph.node_link_data(subgraph))])
    return result


def get_filtered_subgraphs(args):
    """Return subgraphs consisting of frequent vertices and edges.

    Args:
      args: ((sentid, graph), vertice_whitelist, edge_whitelist, min_vertices, max_vertices)

    """
    ((sentid, graph), target_word, word_whitelist, edge_whitelist, min_vertices, max_vertices) = args
    result = set()
    gs = json_graph.node_link_graph(json.loads(graph))
    if all(l["word"] != target_word for v, l in gs.nodes(data=True)):
        return result
    for v, l in gs.nodes(data=True):
        if l["word"] not in word_whitelist:
            gs.remove_node(v)
    for s, t, l in gs.edges(data=True):
        if l["relation"] not in edge_whitelist:
            gs.remove_edge(s, t)
    for comp in networkx.weakly_connected_component_subgraphs(gs):
        if comp.__len__() < min_vertices:
            continue
        if all(l["word"] != target_word for v, l in comp.nodes(data=True)):
            continue
        bfo_graph, bfo_to_raw = None, None
        try:
            bfo_graph, bfo_to_raw = subgraph_enumeration.get_bfo(comp, fragment=True)
        except IndexError:
            continue
        for subgraph in subgraph_enumeration.enumerate_csg_minmax(bfo_graph, bfo_to_raw, min_vertices=min_vertices, max_vertices=max_vertices):
            if all(l["word"] != target_word for v, l in subgraph.nodes(data=True)):
                continue
            subgraph = nx_graph.canonize(subgraph)
            result.update([json.dumps(json_graph.node_link_data(subgraph))])
    return result



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Extract frequent subgraphs')
    parser.add_argument("--min-subgraph", type=int, help="Minimum subgraph size (default: 2 vertices)", default=2, metavar="N")
    parser.add_argument("--max-subgraph", type=int, help="Maximum subgraph size (default: 5 vertices)", default=5, metavar="N")
    parser.add_argument("-s", "--support", type=int, help="Minimum support (default: 50 sentences)", default=50, metavar="N")
    parser.add_argument("-w", "--wordfilter", type=str, required=True, help="Only subgraphs containing a specific word")
    parser.add_argument("-o", "--outfile", type=argparse.FileType("w"), required=True, help="Output file (JSON)")
    parser.add_argument("DB", type=os.path.abspath, help="SQLite3 database")
    args = parser.parse_args()

    groupsize = 50 * 10 * multiprocessing.cpu_count()
    conn, c = connect_to_db(args.DB)
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    sent_count = c.execute("SELECT count(*) FROM sentences").fetchone()[0]
    vertices = collections.Counter()
    edges = collections.Counter()
    words = collections.Counter()
    logging.info("%d sentences" % sent_count)
    logging.info("collect frequency information")
    # for offset in xrange(0, sent_count, groupsize):
    #     sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
    #     logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
    #     r = pool.imap_unordered(get_vertices_and_edges, sentences, 10)
    #     # r = map(get_vertices_and_edges, sentences)
    #     for vs, es in r:
    #         for v in vs: vertices[v] += 1
    #         for e in es: edges[e] += 1
    # vertice_whitelist = set((v for v in vertices if vertices[v] >= args.support))
    # edge_whitelist = set((v for v in edges if edges[v] >= args.support))
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        r = pool.imap_unordered(get_words_and_edges, sentences, 10)
        # r = map(get_vertices_and_edges, sentences)
        for ws, es in r:
            for w in ws: words[w] += 1
            for e in es: edges[e] += 1
    word_whitelist = set((w for w in words if words[w] >= args.support))
    edge_whitelist = set((e for e in edges if edges[e] >= args.support))
    if args.wordfilter not in word_whitelist:
        logging.warning("Support for %s is lower than %d" % (args.wordfilter, args.support))
        exit()
    subgraphs = collections.Counter()
    logging.info("find subgraphs with support >= %d" % args.support)
    # for offset in xrange(0, sent_count, groupsize):
    #     sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
    #     logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
    #     arguments = itertools.izip(sentences, itertools.repeat(vertice_whitelist), itertools.repeat(edge_whitelist), itertools.repeat(args.min_subgraph), itertools.repeat(args.max_subgraph))
    #     r = pool.imap_unordered(get_subgraphs, arguments, 10)
    #     # r = map(get_subgraphs, arguments)
    #     for sgs in r:
    #         for sg in sgs: subgraphs[sg] += 1
    sent_count = c.execute("SELECT count(*) FROM sentences INNER JOIN tokens USING (sentid) INNER JOIN types USING (typeid) WHERE types.word=?", (args.wordfilter, )).fetchone()[0]
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT DISTINCT sentid, graph FROM sentences INNER JOIN tokens USING (sentid) INNER JOIN types USING (typeid) WHERE types.word=? LIMIT ? OFFSET ?", (args.wordfilter, groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        arguments = itertools.izip(sentences, itertools.repeat(args.wordfilter), itertools.repeat(word_whitelist), itertools.repeat(edge_whitelist), itertools.repeat(args.min_subgraph), itertools.repeat(args.max_subgraph))
        r = pool.imap_unordered(get_filtered_subgraphs, arguments, 10)
        # r = map(get_subgraphs, arguments)
        for sgs in r:
            for sg in sgs: subgraphs[sg] += 1
    logging.info("write output file")
    json.dump({k: v for k, v in subgraphs.iteritems() if v >= args.support}, args.outfile)
