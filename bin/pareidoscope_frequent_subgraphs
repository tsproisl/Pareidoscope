#!/usr/bin/python
# -*- coding: utf-8 -*-

import argparse
import collections
import itertools
import json
import logging
import multiprocessing
import os
import sqlite3

import networkx
from networkx.readwrite import json_graph

from pareidoscope import subgraph_enumeration
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def connect_to_db(filename):
    """Connect to an existing database.
    
    Args:
      filename:

    """
    if not os.path.exists(filename):
        raise Exception("Database file does not exist")
    conn = sqlite3.connect(filename)
    c = conn.cursor()
    c.execute("PRAGMA page_size=4096")
    c.execute("PRAGMA cache_size=100000")
    return conn, c


def get_vertices_and_edges(args):
    """Return the vertices and edges of the graph.

    Args:
      args: (sentid, graph)

    """
    sentid, graph = args
    vertices, edges = [], []
    gs = json_graph.node_link_graph(json.loads(graph))
    for vertice in gs.nodes():
        word = gs.node[vertice]["word"]
        pos = gs.node[vertice]["pos"]
        lemma = gs.node[vertice]["lemma"]
        wc = gs.node[vertice]["wc"]
        vertices.append((word, pos, lemma, wc))
    for s, t, l in gs.edges(data=True):
        edges.append(l["relation"])
    return vertices, edges


def get_subgraphs(args):
    """Return subgraphs consisting of frequent vertices and edges.

    Args:
      args: ((sentid, graph), vertice_whitelist, edge_whitelist, min_vertices, max_vertices)

    """
    ((sentid, graph), vertice_whitelist, edge_whitelist, min_vertices, max_vertices) = args
    result = []
    gs = json_graph.node_link_graph(json.loads(graph))
    for vertice in gs.nodes():
        word = gs.node[vertice]["word"]
        pos = gs.node[vertice]["pos"]
        lemma = gs.node[vertice]["lemma"]
        wc = gs.node[vertice]["wc"]
        if (word, pos, lemma, wc) not in vertice_whitelist:
            gs.remove_node(vertice)
    for s, t, l in gs.edges(data=True):
        if l["relation"] not in edge_whitelist:
            gs.remove_edge(s, t)
    for comp in networkx.weakly_connected_components(gs):
        if comp.__len__() < min_vertices:
            continue
        bfo_graph, bfo_to_raw = None, None
        try:
            bfo_graph, bfo_to_raw = subgraph_enumeration.get_bfo(gs, fragment=True)
        except IndexError:
            continue
        for subgraph in subgraph_enumeration.enumerate_csg_minmax(bfo_graph, bfo_to_raw, min_vertices=min_vertices, max_vertices=max_vertices):
            subgraph = nx_graph.canonize(subgraph)
            result.append(json.dumps(json_graph.node_link_data(subgraph)))
    return result



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Extract frequent subgraphs')
    parser.add_argument("--min-subgraph", type=int, help="Minimum subgraph size (default: 2 vertices)", default=2, metavar="N")
    parser.add_argument("--max-subgraph", type=int, help="Maximum subgraph size (default: 5 vertices)", default=5, metavar="N")
    parser.add_argument("-s", "--support", type=int, help="Minimum support (default: 50 sentences)", default=50, metavar="N")
    parser.add_argument("-o", "--outfile", type=argparse.FileType("w"), required=True, help="Output file (JSON)")
    parser.add_argument("DB", type=os.path.abspath, help="SQLite3 database")
    args = parser.parse_args()

    groupsize = 50 * 10 * multiprocessing.cpu_count()
    conn, c = connect_to_db(args.DB)
    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())
    sent_count = c.execute("SELECT count(*) FROM sentences").fetchone()[0]
    vertices = collections.Counter()
    edges = collections.Counter()
    logging.info("%d sentences" % sent_count)
    logging.info("collect frequency information")
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        r = pool.imap_unordered(get_vertices_and_edges, sentences, 10)
        # r = map(get_vertices_and_edges, sentences)
        for vs, es in r:
            for v in vs: vertices[v] += 1
            for e in es: edges[e] += 1
    vertice_whitelist = set((v for v in vertices if vertices[v] >= args.support))
    edge_whitelist = set((v for v in edges if edges[v] >= args.support))
    subgraphs = collections.Counter()
    logging.info("find subgraphs with support > %d" % args.support)
    for offset in xrange(0, sent_count, groupsize):
        sentences = c.execute("SELECT sentid, graph FROM sentences LIMIT ? OFFSET ?", (groupsize, offset)).fetchall()
        logging.info("process sentences %d to %d", offset+1, offset+len(sentences))
        arguments = itertools.izip(sentences, itertools.repeat(vertice_whitelist), itertools.repeat(edge_whitelist), itertools.repeat(args.min_subgraph), itertools.repeat(args.max_subgraph))
        r = pool.imap_unordered(get_subgraphs, arguments, 10)
        # r = map(get_subgraphs, arguments)
        for sgs in r:
            for sg in sgs: subgraphs[sg] += 1
    json.dump(subgraphs, args.outfile)
