#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import itertools
import json
import math
import multiprocessing
import random

import pareidoscope.query
from pareidoscope.utils import cwb
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph


def read_queries(queries_file):
    """Read all queries."""
    queries = [line.rstrip("\n").split("\t") for line in queries_file if not (line.startswith("#") or line == "\n")]
    queries = [[nx_graph.create_nx_digraph(json.loads(q)) for q in l] for l in queries]
    return queries


def check_queries(queries):
    """Do a sanity check on all queries."""
    queries = [q for q in queries if pareidoscope.query.sanity_check_c_a_b(*q)]
    return queries


def derive_gn(queries):
    """For all queries, derive gn from gc, ga and gb."""
    queries = [q + [pareidoscope.query.get_n_from_c_a_b(*q)] for q in queries]
    return queries


def search_choke_points(queries):
    """Search for choke point vertices"""
    queries = [q + [nx_graph.get_choke_point(q[3])] for q in queries]
    return queries


def write_results(prefix, results):
    """Write results to files
    
    Arguments:
    - `prefix`:
    - `results`:
    """
    with open("%s.dat" % prefix, "w") as fh:
        json.dump(results, fh)
    for suffix in results[0].keys():
        with open("%s_%s.mi" % (prefix, suffix), "w") as fh:
            fh.write("MI\n")
            for i, q in enumerate(results):
                s = q[suffix]
                e11 = 0
                if s["o11"] == 0:
                    continue
                if suffix in ["iso_ct", "sub_ct", "choke_point_ct", "sent_ct"]:
                    if any([s[x] == 0 for x in ["r1", "c1", "n"]]):
                        e11 = 0
                    else:
                        e11 = (s["r1"] * s["c1"]) / float(s["n"])
                else:
                    pass
                mi = math.log(s["o11"]/float(e11), 2)
                fh.write("%d\t%f\n" % (i+1, mi))

                

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run a batch of queries against a corpus')
    parser.add_argument("-c", "--corpus", type=argparse.FileType("r"), required=True, help="Corpus in CWB format")
    parser.add_argument("-o", "--output", type=str, required=True, help="Output prefix")
    parser.add_argument("QUERIES", type=argparse.FileType("r"), help="Queries file with G_C\tG_A\tG_B")
    args = parser.parse_args()

    groupsize = 5 * 50 * multiprocessing.cpu_count()
    
    queries = read_queries(args.QUERIES)
    queries = check_queries(queries)
    queries = derive_gn(queries)
    queries = search_choke_points(queries)
    # for l in queries:
    #     for q in l[:-1]:
    #         print(nx_graph.export_to_adjacency_matrix(q))
    
    results = [{} for q in queries]
    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())
    # sents = cwb.sentences_iter(args.corpus)
    # # for testing purposes only:
    # # sents = helper.sample(sents, 0.0005)
    # for sentences in helper.grouper_nofill(groupsize, sents):
    #     r = pool.imap_unordered(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(queries)), 50)
    #     # r = list(map(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(queries))))
    #     for result, sensible in r:
    #         if sensible:
    #             pareidoscope.query.merge_result(result, results)
    # write_results(args.output, results)

    sentences = cwb.sentences_iter(args.corpus)
    r = pool.imap_unordered(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(queries)), 50)
    for result, sensible in r:
        if sensible:
            pareidoscope.query.merge_result(result, results)
    write_results(args.output, results)
