#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import itertools
import json
import logging
import math
import multiprocessing

from networkx.readwrite import json_graph

import pareidoscope.query
from pareidoscope.utils import cwb
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)


def arguments():
    """"""
    parser = argparse.ArgumentParser(description='Run a batch of queries against a corpus')
    parser.add_argument("-c", "--corpus", type=argparse.FileType("r"), required=True, help="Corpus in CWB format")
    parser.add_argument("-o", "--output", type=str, required=True, help="Output prefix")
    parser.add_argument("QUERIES", type=argparse.FileType("r"), help="Queries file with one graph per line and vertices marked as belonging to A, B or both (AB)")
    return parser.parse_args()


def read_queries(queries_file):
    """Read all queries."""
    for line in queries_file:
        if line.startswith("#") or line == "\n":
            continue
        yield json_graph.node_link_graph(json.loads(line))


def extract_graphs(query):
    """Extract query graphs for A, B and N"""
    a = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "A" or l["query"] == "AB"])
    b = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "B" or l["query"] == "AB"])
    n = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "AB"])
    for v, l in query.nodes(data=True):
        l["vid"] = v
        del l["query"]
    return [nx_graph.ensure_consecutive_vertices(g) for g in (query, a, b, n)]


def add_focus_point(query):
    """Search for choke point vertices"""
    focus_point_vertex = nx_graph.get_choke_point(query[3])
    # focus_point_vid = query[3].node[focus_point_vertex]["vid"]
    return query + [focus_point_vertex]


def write_results(prefix, results):
    """Write results to files

    Arguments:
    - `prefix`:
    - `results`:
    """
    with open("%s.dat" % prefix, "w") as fh:
        json.dump(results, fh)
    for suffix in results[0].keys():
        with open("%s_%s.mi" % (prefix, suffix), "w") as fh:
            fh.write("MI\n")
            for i, q in enumerate(results):
                s = q[suffix]
                e11 = 0
                if s["o11"] == 0:
                    continue
                if suffix in ["iso_ct", "sub_ct", "choke_point_ct", "sent_ct"]:
                    if any([s[x] == 0 for x in ["r1", "c1", "n"]]):
                        e11 = 0
                    else:
                        e11 = (s["r1"] * s["c1"]) / float(s["n"])
                else:
                    pass
                mi = math.log(s["o11"]/float(e11), 2)
                fh.write("%d\t%f\n" % (i+1, mi))


def main():
    """"""
    args = arguments()
    queries = read_queries(args.QUERIES)
    queries = [extract_graphs(q) for q in queries]
    queries = [add_focus_point(q) for q in queries]
    # print(queries)
    # print([[json.dumps(json_graph.node_link_data(g)) for g in q[:3]] for q in queries])

    groupsize = 5 * 50 * multiprocessing.cpu_count()

    results = [{} for q in queries]
    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())
    sents = cwb.sentences_iter(args.corpus)
    # for testing purposes only:
    # sents = helper.sample(sents, 0.0005)
    i = 0
    for sentences in helper.grouper_nofill(groupsize, sents):
        i += 1
        logging.info("slice no. %d" % i)
        r = pool.imap_unordered(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(queries)), 50)
        # r = map(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(queries)))
        for result, sensible in r:
            if sensible:
                pareidoscope.query.merge_result(result, results)
    write_results(args.output, results)

    # sentences = cwb.sentences_iter(args.corpus)
    # r = pool.imap_unordered(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(queries)), 50)
    # # r = map(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(queries)))
    # for result, sensible in r:
    #     if sensible:
    #         pareidoscope.query.merge_result(result, results)
    # write_results(args.output, results)


if __name__ == "__main__":
    main()
