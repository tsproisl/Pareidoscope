#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import functools
import itertools
import json
import logging
import multiprocessing
import os

from networkx.readwrite import json_graph

import pareidoscope.query
from pareidoscope.utils import conllu
from pareidoscope.utils import cwb
from pareidoscope.utils import database
from pareidoscope.utils import helper
from pareidoscope.utils import nx_graph
from pareidoscope.utils import statistics

logging.basicConfig(format="%(levelname)s %(asctime)s: %(message)s", level=logging.INFO)

queries = []


def arguments():
    """"""
    parser = argparse.ArgumentParser(description='Run a batch of queries against a corpus')
    parser.add_argument("-a", help="Determine frequencies for all counting methods and not only for focus points", action='store_true')
    parser.add_argument("-f", "--format", choices=["conllu", "cwb", "db"], required=True, help="Input format of the corpus: Either a text-based format (CoNLL-U or CWB-treebank) or a database created by pareidoscope_corpus_to_sqlite")
    parser.add_argument("-o", "--output", type=str, required=True, help="Output prefix")
    parser.add_argument("-p", "--cpu", type=int, default=25, help="Percentage of CPUs to use (0-100; default: 25)")
    parser.add_argument("CORPUS", type=os.path.abspath, help="Input corpus")
    parser.add_argument("QUERIES", type=argparse.FileType("r", encoding="utf-8"), help="Queries file with one graph per line and vertices marked as belonging to A, B or both (AB)")
    return parser.parse_args()


def read_queries(queries_file):
    """Read all queries."""
    queries = json.load(queries_file)
    for query in queries:
        yield json_graph.node_link_graph(query, directed=True, multigraph=False)
    # for line in queries_file:
    #     if line.startswith("#") or line == "\n":
    #         continue
    #     yield json_graph.node_link_graph(json.loads(line))


def remove_auxiliary_attributes(graph, abcn):
    """Remove auxiliary attributes"""
    for v, l in graph.nodes(data=True):
        l["vid"] = v
        if abcn == "a" or abcn == "n":
            if "only_B" in l:
                for attribute in l["only_B"]:
                    del l[attribute]
        elif abcn == "b" or abcn == "n":
            if "only_A" in l:
                for attribute in l["only_A"]:
                    del l[attribute]
        if abcn != "n":
            if "focus_point" in l:
                del l["focus_point"]
        del l["query"]
        if "only_A" in l:
            del l["only_A"]
        if "only_B" in l:
            del l["only_B"]
    return graph


def extract_graphs(query):
    """Extract query graphs for A, B and N"""
    a = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "A" or l["query"] == "AB"]).copy()
    b = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "B" or l["query"] == "AB"]).copy()
    n = query.subgraph([v for v, l in query.nodes(data=True) if l["query"] == "AB"]).copy()
    ga = remove_auxiliary_attributes(a, "a")
    gb = remove_auxiliary_attributes(b, "b")
    gn = remove_auxiliary_attributes(n, "n")
    gc = remove_auxiliary_attributes(query, "c")
    return [nx_graph.ensure_consecutive_vertices(g) for g in (gc, ga, gb, gn)]


def sentence_candidates(c, g):
    """Get candidate sentences (no token candidates!) for the query
    graph.

    """
    sentence_ids = []
    for vertex in g.nodes():
        sql_query = "SELECT DISTINCT sentence_id FROM tokens WHERE "
        where = []
        args = []
        pos_lexical = set(["word", "pos", "lemma", "wc", "root"])
        neg_lexical = set(["not_%s" % pl for pl in pos_lexical])
        indegree = g.in_degree(vertex)
        outdegree = g.out_degree(vertex)
        if indegree > 0:
            where.append("indegree >= ?")
            args.append(indegree)
        if outdegree > 0:
            where.append("outdegree >= ?")
            args.append(outdegree)
        for k, v in g.node[vertex].items():
            if k in pos_lexical:
                where.append("%s = ?" % k)
                if k == "root":
                    args.append(v == "root")
                else:
                    args.append(v)
            elif k in neg_lexical:
                k = k[4:]
                where.append("%s != ?" % k)
                if k == "root":
                    args.append(v == "root")
                else:
                    args.append(v)
            elif k == "not_indep":
                relations = []
                for rel in v:
                    relations.append("relation = ?")
                    args.append(rel)
                where.append("NOT EXISTS (SELECT 1 FROM dependencies WHERE dependent_id = token_id AND (%s))" % " OR ".join(relations))
            elif k == "not_outdep":
                relations = []
                for rel in v:
                    relations.append("relation = ?")
                    args.append(rel)
                where.append("NOT EXISTS (SELECT 1 FROM dependencies WHERE governor_id = token_id AND (%s))" % " OR ".join(relations))
            else:
                raise Exception("Unsupported key: %s" % k)
        sql_query += " AND ".join(where)
        sentence_ids.append(set([r[0] for r in c.execute(sql_query, args).fetchall()]))
    candidate_ids = functools.reduce(lambda x, y: x.intersection(y), sentence_ids)
    candidate_sentences = [r[0] for r in c.execute("SELECT graph FROM sentences WHERE sentence_id IN (%s)" % ", ".join([str(_) for _ in candidate_ids])).fetchall()]
    return candidate_sentences


def add_focus_point(query):
    """Search for choke point vertices"""
    gc, ga, gb, gn = query
    focus_point_vertex = None
    for v, l in gn.nodes(data=True):
        if "focus_point" in l:
            focus_point_vertex = l["focus_point"]
            del l["focus_point"]
            break
    if focus_point_vertex is None:
        focus_point_vertex = nx_graph.get_choke_point(gn)
    return gc, ga, gb, gn, focus_point_vertex


def write_results(prefix, results):
    """Write results to files

    Arguments:
    - `prefix`:
    - `results`:
    """
    with open("%s.json" % prefix, "w") as fh:
        json.dump(results, fh, ensure_ascii=False, indent=4, sort_keys=True)


def main():
    """"""
    args = arguments()
    queries = read_queries(args.QUERIES)
    queries = [extract_graphs(q) for q in queries]
    queries = [add_focus_point(q) for q in queries]
    results = [{} for q in queries]
    cpu_count = multiprocessing.cpu_count()
    processes = min(max(1, int(cpu_count * args.cpu / 100)), cpu_count)
    groupsize = 10 * 10 * processes
    with multiprocessing.Pool(processes=processes) as pool:
        logging.info("using %d cpus" % processes)
        if args.format == "db":
            conn, c = database.connect_to_database(args.CORPUS)
            for i, qline in enumerate(queries):
                logging.info("query no. %d" % i)
                gc, ga, gb, gn, choke_point = qline
                sents = sentence_candidates(c, pareidoscope.query.strip_vid(gn))
                logging.info("%d candidate sentences" % len(sents))
                for sentences in helper.grouper_nofill(groupsize, sents):
                    query_args = (itertools.chain.from_iterable(a) for a in zip(itertools.repeat(qline), ((s,) for s in sentences), ((a,) for a in itertools.repeat(args.a))))
                    r = pool.imap_unordered(pareidoscope.query.run_queries_db, query_args, 10)
                    # for debugging, it is often better to avoid multiprocessing:
                    # r = map(pareidoscope.query.run_queries_db, query_args)
                    for result in r:
                        pareidoscope.query.merge_result_db(result, i, results)
                logging.info(results[i])
        else:
            with open(args.CORPUS, encoding="utf-8") as corpus:
                if args.format == "cwb":
                    sents = cwb.sentences_iter(corpus, return_id=False)
                elif args.format == "conllu":
                    sents = conllu.sentences_iter(corpus, return_id=False)
                for i, sentences in enumerate(helper.grouper_nofill(groupsize, sents)):
                    logging.info("processing sentences %d--%d" % (i * groupsize + 1, i * groupsize + min(groupsize, len(sentences))))
                    r = pool.imap_unordered(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(args.format), itertools.repeat(queries), itertools.repeat(args.a)), 10)
                    # for debugging, it is often better to avoid multiprocessing:
                    # r = map(pareidoscope.query.run_queries, zip(sentences, itertools.repeat(args.format), itertools.repeat(queries)))
                    for result, sensible in r:
                        if sensible:
                            pareidoscope.query.merge_result(result, results)
        for result in results:
            for counting_method, freq in result.items():
                o, e = statistics.get_contingency_table(freq["o11"], freq["r1"], freq["c1"], freq["n"])
                freq["log_likelihood"] = statistics.one_sided_log_likelihood(o, e)
                freq["t_score"] = statistics.t_score(o, e)
                freq["dice"] = statistics.dice(o, e)
        write_results(args.output, results)


if __name__ == "__main__":
    main()
